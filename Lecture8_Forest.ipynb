{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8beeed",
   "metadata": {},
   "source": [
    "# ECON5280 Lecture 8 Causal Forest\n",
    "\n",
    "<font size=\"5\">Junlong Feng</font>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junlong-feng/econ5280/main?filepath=Lecture8_Forest.ipynb)\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Motivation: Knowing heterogeneity is great and we hate assumptions on functional form.\n",
    "* CATE and Heterogeneity: Why is heterogeneity interesting but hard to know?\n",
    "* Traditional Methods and Their Drawbacks: A brief intro/review of nonparametrics.\n",
    "* Random Tree, Causal Tree and Causal Forest: Econometrics learns from and develops ML.\n",
    "* Applications and Implementation: Everything can be done in R by some simple commands.\n",
    "\n",
    "## 1. CATE and Heterogeneity\n",
    "\n",
    "We learned CATE in Lecture 5. Let $D$ be the treatment, $W$ be a vector of control variables, and $Y$ be the outcome, recall that CATE from $D=d'$ to $d$ is defined as:\n",
    "$$\n",
    "CATE(d,d';W)\\equiv\\mathbb{E}(Y(d)|W)-\\mathbb{E}(Y(d')|W),\n",
    "$$\n",
    "where $Y(d)$ is the potential outcome at $d$. \n",
    "\n",
    "Knowing CATE is great because\n",
    "\n",
    "- You can get ATE by $ATE(d,d')=\\mathbb{E}[CATE(d,d';W)]$.\n",
    "- You can get the so-called *heterogeneous effects*: People with different values of $W$ may have different treatment effects. Closer to the notion of *individual treatment effect*.\n",
    "  - Let $D$ be attending graduate school or not. Let $W$ contain family income, gender, college major.\n",
    "  - The expected income effect of attending graduate school may be different for a girl from a high-income family with econ major compared to the effect for a boy from a low-income family with non-econ major. ATE averages everything out, but CATE tells you the heterogeneity.\n",
    "\n",
    "Knowing CATE is possible because\n",
    "\n",
    "- Under conditional random assignment: $D\\perp Y(d)|W$ for all $d$, we have CATE identified as:\n",
    "  $$\n",
    "  CATE(d,d';W)=\\mathbb{E}(Y|D=d,W)-\\mathbb{E}(Y|D=d',W).\n",
    "  $$\n",
    "\n",
    "- The two conditional expectations are directly identified and estimable from data.\n",
    "\n",
    "- We said in Lecture 5 that you can make it linear if you're willing to assume the expectation is linear in $W$.\n",
    "\n",
    "- But this can be a crazy assumption.\n",
    "\n",
    "Knowing CATE is hard because\n",
    "\n",
    "- Estimating unconditional expectation is easy: Take average and done.\n",
    "- Estimating conditional expectation when the conditioning variable is discrete is easy as well. \n",
    "  - For instance, let $W\\in\\{0,1\\}$. Then $\\mathbb{E}(Y|W=1)$ can be estimated by taking the average of $Y_{i}$s for the **subsample** where $X_{i}=1$: $\\sum_{i:W_{i}=1}Y_{i}/\\#(i:W_{i}=1)$.\n",
    "- Estimating conditional expectation when the conditioning variable is continuous is hard. \n",
    "  - When the conditioning variable $W$ is continuous, $W=w$ has probability 0 for any $w$.\n",
    "  - Impossible (with probability one) to find a **subgroup** whose $W_{i}=w$.\n",
    "  - For instance, suppose $W$ is annual income. Suppose I ask you to estimate $\\mathbb{E}(Y|W=10125)$, i.e., the expected $Y$ for someone whose annual income is exactly equal to 10125. It is highly possible that in your data set there does not exist any individual whose income is equal to this number.\n",
    "\n",
    "## 2. Traditional Methods and Their Drawbacks\n",
    "\n",
    "Now let's forget about treatment effect for a moment. The conditional expectation is a function of the conditioning variables. For instance, $\\mathbb{E}(Y|D=1,W)$ changes when $W$ changes. So if we recover the entire function: $\\mathbb{E}(D=1,W=w)$ for all possible $w$ that $W$ can take, then we are done. For notational simplicity, let\n",
    "$$\n",
    "f_{d}(w)\\equiv \\mathbb{E}(Y|D=d,W=w).\n",
    "$$\n",
    " Our goal is to estimate function $f_{d}(w)$ for different $d$s.\n",
    "\n",
    "Traditional methods estimate $f_{d}(w)$ using $Y$s around $W=w$. For instance, suppose I assme funtion $f_{d}(w)$ is continuous in $w$, then if I go arbitrarily close to $w$, the function value will also be arbitrarily close to $f_{d}(w)$.\n",
    "\n",
    "- Suppose $W$ is a scalar and is continuous.\n",
    "- Suppose we want to know $f_{d}(10)$.\n",
    "- Ideally we would like to find all $Y_{i}$s whose $W_{i}=10$ and then take average.\n",
    "- But this may not be possible. So instead, we find all $i$s whose $W_{i}\\in (10-h,10+h)$, and average over their $Y_{i}$s.\n",
    "- Of course there is bias because, say, $f_{d}(9.8)$, is not equal to $f_{d}(10)$ even though they are close. \n",
    "- Bias is larger if $h$ is larger. Variance is smaller is $h$ is larger because the effective sample size increases.\n",
    "- If I send $h\\to 0$ when $n\\to\\infty $, the bias should be gone asymptotically and I get a consistent estimate.\n",
    "\n",
    "This is the central idea of the traditional **nonparametric methods** in statistics.\n",
    "\n",
    "- Key logic: When $w$ changes a little, $f_{d}(w)$ changes a little.\n",
    "- Key task: Find observations whose $W$'s values are close to the target $w$.\n",
    "- Key challenge: **Curse of dimensionality**.\n",
    "\n",
    "**Curse of dimensionality**. When $W$ is no longer a scalar, say it contains $p>1$ variables, it's hard to find *nearby* observations:\n",
    "\n",
    "- Suppose you have $n$ data points, i.e., $n$ individuals.\n",
    "- Suppose all variables in $W$ take values on $[0,1]$.\n",
    "- When $p=1$, it's like throwing $n$ points randomly into a unit interval. Points are close to each other.\n",
    "- When $p=2$, throw $n$ points into a unit square. When $p=3$, a cube. And so on...\n",
    "- On average, your $n$ data points lie farther away from each other as $p$ gets larger, creating the so-called **sparsity**.\n",
    "- For instance, suppose $W$ only contains income. Then for $W=10125$, you only need to find people whose income is around 10125. But if $W$ contains income and experience, and for $W=(10125,10)$, you need to find people whose income is around 10125 **and** whose experience is around 10 years. Sounds more difficult right?\n",
    "\n",
    "## 3. Random Tree, Causal Tree, and Causal Forest\n",
    "\n",
    "The key problem behind the curse of dimensionality is that we are obsessed with the idea of finding $i$ whose $W$ is close to $w$. However, this is sufficient but not necessary to approximate $f_{d}(w)$.\n",
    "\n",
    "- It is possible that $f_{d}(w)=f_{d}(w')$ when $w$ and $w'$ are far away. \n",
    "- So only focusing on points near $w$ loses information.\n",
    "\n",
    "It would be better if we find individuals whose $f_{d}(w)$ are similar, and average over their $Y_{i}$s. But how is this possible as $f_{d}(w)$ is unknown and we are going to estimate it?\n",
    "\n",
    "### 3.1 Random Tree\n",
    "\n",
    "Now suppose $p=2$, i.e., there are two variables in $W$: $W_{1}$ and $W_{2}$. Further, imagine $(W_{1},W_{2})$ take values on the unit square $[0,1]\\times [0,1]$.\n",
    "\n",
    "- Now forget about the target value $w$ and we'll never try to find values close to $w$ any more.\n",
    "- Instead, we are going to split the unit square into small rectangles.\n",
    "- The goal is, in each rectangle, the conditional expectation, or, $f_{d}(w)$, is almost a constant.\n",
    "- After we finish, we go back and find out in which rectangle our target $w$ lies. Then simply average $Y_{i}$s for the $i$s whose $W$ take value in the rectangle.\n",
    "\n",
    "Two problems:\n",
    "\n",
    "1. How do we split the unit square efficiently?\n",
    "   - Infinite ways to split it.\n",
    "2. How do we know whether $f_{d}(w)$ changes a lot in a rectangle or not? \n",
    "   - We need to estimate $f_{d}(w)$ by taking average of the $Y$s in a rectangle, then by construction its estimate does not change at all for all $w$ in the rectangle.\n",
    "\n",
    "**The random tree algorithm** (a skecth).\n",
    "\n",
    "1. Split the sample into two halves by $W_{1}<t_{1}$ and $W_{1}>t_{1}$. Calculate the average of $Y_{i}$s in these two subsamples. Denote them by $\\bar{Y}_{t1}^{(1)}$ and $\\bar{Y}_{t1}^{(2)}$. Calculate $\\Delta_{1}(t_{1})\\equiv (\\bar{Y}_{t1}^{(1)}-\\bar{Y}_{t1}^{(2)})^{2}$. \n",
    "2. Try all possible $t_{1}\\in (0,1)$. Find the one that yields the largest $\\Delta_{1}(t_{1})$. Call it $t_{1}^{*}$. Mathematically, $t_{1}^{*}=\\arg\\max_{t_{1}}\\Delta_{1}(t_{1})$.\n",
    "3. Split the **original** sample into two halves by $W_{2}<t_{2}$ and $W_{2}>t_{2}$. Repeat Steps 1 and 2 for all possible $t_{2}\\in (0,1)$ and find $t_{2}^{*}$.\n",
    "4. Compare $\\Delta_{1}(t_{1}^{*})$ and $\\Delta_{2}(t_{2}^{*})$. We choose **the regressor**, 1 or 2, and its corresponding $t^{*}$ which yields the largest $\\Delta(t^{*})$ as our first-round split rule.\n",
    "5. Now we have two subsamples. In each of them, repeat Steps 1-4.\n",
    "6. Repeat the splitting until the pre-set number of rounds splitting and/or the size of the final rectangles are reached. The final rectangles are called nodes or **leaves**.\n",
    "\n",
    "Notes. If you learned random tree from ML, you may remember one needs a testing set. This algorithm is simpler by taking advantage of the splitting criterion we use. The idea is from Wager and Athey (2018, JASA) and Athey, Tibshirani and Wager (2019, Annals of Stats). Feel free to explore the difference if you’re interested but this is not required by this course.\n",
    "\n",
    "This algorithm solves the two questions we raised earlier:\n",
    "\n",
    "1. How do we split the unit square efficiently?\n",
    "\n",
    "   - No matter how large $p$ is, each time we only focus on one of them. Each splitting trial is a simple one-dimensional splitting problem. Very easy to handle; the complexity of the problem is linear in $p$.\n",
    "   - The above algorithm is for $p=2$. When $p>2$, only needs to add similar steps as step 3 for every covariate. Then in step 4, compare $\\Delta_{1}(t_{1}^{∗})$, $\\Delta_{2}(t_{2}^{∗})$, ..., and $\\Delta_{p}(t_{p}^{∗})$, and find the largest one.\n",
    "   - This **recursive splitting** algorithm is so simple that can be represented by a binary tree shown below. \n",
    "\n",
    "2. How do we know whether $f_{d}(w)$ changes a lot in a leaf or not? \n",
    "\n",
    "   - We actually do not look at $f_{d}(W)$ i.e. $E(Y|W_{1},W_{2})$ on one leaf. We compare the difference between the $f_{d}(W)$s at different leaves. We find the split that yields that largest difference (largest contrast, as some authors prefer). \n",
    "   - In this way, the points on one leaf A have relatively similar function value $f_{d}(\\cdot)$ compared with the points in another leaf B. Otherwise, those points would have been classfied onto leaf B.\n",
    "   - When the leaves are small enough, we are confident that function $f_{d}(\\cdot)$ is almost constant on it, leading to small enough bias when averaging $Y_{i}$s on it.\n",
    "   \n",
    "### 3.2 Causal Tree\n",
    "\n",
    "So far we used tree to estimate conditional expectations. That works well based on ML theory. Econometrics plays no role in it yet. \n",
    "\n",
    "However, our ultimate goals are i) to estimate causal effect and ii) conduct statistical inference. These are two new questions to answer besides the two in [Section 3.1](#3.1 Random Tree). \n",
    "\n",
    "1. How do we estimate CATE?\n",
    "2. How do we establish statistical properties like consistency and asymptotic distribution?\n",
    "\n",
    "#### 3.2.1 Changing the Splitting Critirion\n",
    "\n",
    "In the tree algorithm, we treated $\\mathbb{E}(Y|D=d,W=w)$ as a function of $w$, i.e., $f_{d}(w)$. We set splitting criterion by comparing $\\hat{f}_{d}$ for $W_{j}>t_{j}$ and $W_{j}<t_{j}$, and set the split as the $j$ and $t_{j}$ that yields the largest contrast.\n",
    "\n",
    "Now since our goal is to estimate $\\mathbb{E}(Y|D=d,W=w)-\\mathbb{E}(Y|D=d',W=w)$, we can simply treat this difference as the function we care about:\n",
    "\n",
    "- Let $f_{d,d'}(w)\\equiv \\mathbb{E}(Y|D=d,W=w)-\\mathbb{E}(Y|D=d',W=w)$.\n",
    "- Revise Step 1 in the algorithm in [Section 3.1](#3.1 Random Tree) as follows:\n",
    "  1. Split the sample into two halves by $W_{1}<t_{1}$ and $W_{1}>t_{1}$. In each subsample, calculate the average of $Y$ for those observations with $D=d$ subtracted by the average of $Y$ for those observations with $D=d'$. Denote them by $\\bar{Y}_{d-d',t1}^{(1)}$ and $\\bar{Y}_{d-d',t2}$. Calculate $\\Delta_{1}(t_{1})\\equiv (\\bar{Y}_{d-d',t1}-\\bar{Y}_{d-d',t1}^{2})^{2}$. \n",
    "- Then for all the following steps, revise $\\Delta$ in the same way.\n",
    "\n",
    "In this way, we can guarantee that on each leaf, i.e., the final rectangle, the CATE does not change much so treating averaging the $Y$s on it does not create much bias.\n",
    "\n",
    "#### 3.2.2 Honesty\n",
    "\n",
    "The second problem is much harder to solve. It took the academia nearly two decades to come up with a solution.\n",
    "\n",
    "- Stefan Wager and Susan Athey in their 2016 *Proceedings of the National Academy of Sciences of the United States of America (PNAS)* paper solves the problem by introducing a notion called “honesty”.\n",
    "- Recall in the algorithm, $Y$ is used for two purposes: a) determine the covariate and the split in each round, and b) after the leaves are formed, compute the final estimates of CATE.\n",
    "- Such dependence causes theoretical challenges for statistical properties.\n",
    "  - For instance, consistency could be derived by applying WLLN on each leaf: at the end of the day, we just do sample average on each leaf.\n",
    "  - WLLN needs i.i.d.\n",
    "  - However, these leafs are formed by comparing $Y$s. So each leaf is a function of $Y$. Conditional on the leaf, $Y$ are no longer i.i.d.\n",
    "\n",
    "To resolve this issue, Wager and Athey propose an extra step before Step 1 in the algorithm in [Section 3.1](#3.1 Random Tree), called *honest splitting*:\n",
    "\n",
    "**The causal tree algorithm** (a skecth).\n",
    "\n",
    "- Step 0 (Honest splitting). Randomly split the data set into two halves by $i$ (**NOT BY $W$**!)\n",
    "  - Each subsample has around $n/2$ observations. Call them the **training data** and **estimation data** respectively.\n",
    "- Step 1-6. Grow a random tree **using the training data ONLY** following the algorithm in [Section 3.1](#3.1 Random Tree) with the modified $\\Delta$ in [Section 3.2.1](#3.2.1 Changing the Splitting Critirion). \n",
    "- Given the leaves, for $w$ of interest, estimate $CATE(d,d';w)$ by first check which leaf $w$ falls into, and then calculate $\\bar{Y}_{D=d}-\\bar{Y}_{D=d'}$ **using the estimation data**.\n",
    "\n",
    "*Honesty* means that we use independent subsamples to grow a tree and to estimate, respectively. Then the bias caused by “Y is correlated to Y” can be avoided.\n",
    "\n",
    "- Wager and Athey show that under some regularity conditions, the estimated CATE is unbiased, consistent and asymptotically normal. Inference can be easily done by standard t-test, p-value and confidence interval.\n",
    "\n",
    "### 3.3 Causal Forest\n",
    "\n",
    "A causal tree has, again, two drawbacks:\n",
    "\n",
    "1. It’s sensitive to noises.\n",
    "2. Some data (the estimation data) are never used to grow a tree, while some (the training data) are never used for estimation. Information loss.\n",
    "\n",
    "**Causal Forest**: Grow many trees and take the average.\n",
    "\n",
    "**The causal forest algorithm** (a skecth).\n",
    "\n",
    "1. Randomly draw $s$ observations from the full sample of $n$ observations.\n",
    "2. Grow a causal tree using this subsample with the algorthm in [Section 3.2.2](#3.2.2 Honesty).\n",
    "   - The steps are exactly what we described earlier. Recall the key steps are: splitting this subsample into two halves (honesty), recursively splitting the training data to grow a tree, and use the estimation data to estimate the CATE.\n",
    "3. Repeat Steps 1 and 2 for $B$ times. Choose $B$ as large as possible.\n",
    "4. You end up with B causal trees. They are called a **causal forest**. For any given $w$, each tree $b$ yields an estimate of $CATE^{(b)}(d,d';w)$. Take average of theses $B$ estimates: $\\sum_{b=1}^{B}CATE^{(b)}(d,d';w)/B$.\n",
    "\n",
    "The causal forest resolves the two drawbacks of causal trees by:\n",
    "\n",
    "1. It’s sensitive to noises.\n",
    "   - Averaging makes the noise in each tree only have $1/B$ share of impact.\n",
    "2. Some data (the estimation data) are never used to grow a tree, while some (the training data) are never used for estimation. Information loss.\n",
    "   - Training data in one tree may be used as estimation data in another tree. When B is large enough, all data are likely to be used to grow trees and to estimate.\n",
    "\n",
    "Wager and Athey show that under certain regularity conditions: CATE estimated by causal forest is consistent and asymptotically normal.\n",
    "\n",
    "#### 3.3.1 Implementation Details\n",
    "\n",
    "Pay attention to the following three aspects in implementation.\n",
    "\n",
    "**Covariates subsetting**. \n",
    "\n",
    "Recall that when growing a tree, in each round in the recursive splitting, one needs to try every possible split for every covariate. \n",
    "\n",
    "- This can be slow when $p$ is large.\n",
    "- In practice, for each round in the recursive splitting, one only need to try out a random subset of covariates. \n",
    "- This subset is randomly chosen.\n",
    "- A rule of thumb is randomly choosing $\\min⁡\\{\\sqrt{p}+20,p\\}$ covariates.\n",
    "\n",
    "**Minimum leaf size**.\n",
    "\n",
    "Leaf size is like $h$ in traditional nonparametrics. If a leaf is too large, large bias (because extrapolated too much). If a leaf is too small, large variance (because data points are too few).\n",
    "\n",
    "- We can control the minimum size so that when it is reached, recursive splitting ends.\n",
    "- This size can be cross-validated.\n",
    "\n",
    "**Imbalance of a split**.\n",
    "\n",
    "- When splitting a parent node, the size of each child node is not allowed to be too different.\n",
    "- Meanwhile, the number of treated and untreated observations in a child node can not be too different either.\n",
    "- The level of imbalance can be controlled in the algorithm.\n",
    "\n",
    "### 3.4 Comparison with OLS\n",
    "\n",
    "Recall that OLS has the following drawbacks:\n",
    "\n",
    "- You have to assume everything is linear. When your $D$ is binary and you care about ATE, that's fine. But in almost all other cases, this can be a crazy assumption.\n",
    "- Even if the true model is indeed linear, in order to get a consistent estimate of $\\beta$ in front of $D$, both $D$ and $W$ need to be exogenous in general.\n",
    "\n",
    "Causal forest overcomes these drawbacks because i) it does not impose any restrictions on the functional form and ii) everything works under conditional randomization of $D$ whereas $W$ can be arbitrarily correlated with the unobservable(s).\n",
    "\n",
    "## 4. Applications and Implementation\n",
    "\n",
    "You can imagine there is a wide applications of causal forest in economics. Whenever you have an exogenous $D$ and want to know the heterogeneous effects, you can replace linear models (or, any parametric nonlinear models) with it and obtain everything you want. \n",
    "\n",
    "Moreover, you can use it with an instrumental variable as well! \n",
    "\n",
    "* Recall that CLATE when IV and the treatment are both binary is identified as the ratio of two conditional expectation differences. \n",
    "* You can imagine it's not hard to adapt the algorithm to estimate this quantity.\n",
    "  * Already done in the literature.\n",
    "\n",
    "Implementation is also straightfoward because an R package is ready at (check out their tutorial webpage https://grf-labs.github.io/grf/index.html). \n",
    "\n",
    "Here's an example.\n",
    "\n",
    "- I generate a data set with $p=10$ and $n=2000$.\n",
    "- The $2000\\times 10$ matrix of $W$ are randomly drawn from normal.\n",
    "- Treatment $D$ is conditional randomized: It's binary and equal to 1 with probability 0.4 if $X_{1}<0$ and with probability 0.6 if $X_{1}>0$.\n",
    "- The model for $Y$ is nonlinear: $Y=\\max\\{W_{1},0\\}\\times D+W_{2}+\\min\\{W_{3},0\\}+\\varepsilon$.\n",
    "  - Can you calculate the true CATE and ATE by hand? Try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(grf)\n",
    "library(DiagrammeR)\n",
    "### Generate data. You do not need to run these if a dataset is already given. \n",
    "n <- 2000      \n",
    "p <- 10\n",
    "W <- matrix(rnorm(n * p), n, p)\n",
    "D <- rbinom(n, 1, 0.4 + 0.2 * (W[, 1] > 0))\n",
    "Y <- pmax(W[, 1], 0) * D + W[, 2] + pmin(W[, 3], 0) + rnorm(n)\n",
    "data=data.frame(Y,D,W)\n",
    "\n",
    "## Generate values of interest for w\n",
    "W.test <- matrix(0, 101, p)\n",
    "W.test[, 1] <- seq(-2, 2, length.out = 101)\n",
    "\n",
    "### Build a causal forest.\n",
    "tau.forest <- causal_forest(W, Y, D) # Put regressors\n",
    "tree <- get_tree(tau.forest, 1)\n",
    "plot(tree) # visualize the first tree in the forest\n",
    "\n",
    "### Estimate CATE at specified w values using the forest\n",
    "tau.hat <- predict(tau.forest, W.test) \n",
    "plot(W.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), \n",
    "     xlab = \"w\", ylab = \"tau\", type = \"l\") # estimated CATE\n",
    "lines(W.test[, 1], pmax(0, W.test[, 1]), col = 2, lty = 2) # true CATE\n",
    "\n",
    "# Estimate treatment effects with confidence interval\n",
    "tau.forest <- causal_forest(W, Y, D, num.trees = 4000)\n",
    "tau.hat <- predict(tau.forest, W.test, estimate.variance = TRUE)\n",
    "sigma.hat <- sqrt(tau.hat$variance.estimates)\n",
    "plot(W.test[, 1], tau.hat$predictions, ylim =\n",
    "       range(tau.hat$predictions + 1.96 * sigma.hat, \n",
    "             tau.hat$predictions - 1.96 * sigma.hat, 0, 2), \n",
    "     xlab = \"w\", ylab = \"tau\", type = \"l\")\n",
    "lines(W.test[, 1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)\n",
    "lines(W.test[, 1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)\n",
    "lines(W.test[, 1], pmax(0, W.test[, 1]), col = 2, lty = 1)\n",
    "\n",
    "## As promised, ATE can be obtained from CATE\n",
    "average_treatment_effect(tau.forest, target.sample = \"all\")\n",
    "\n",
    "## This is what you'll get if you do OLS\n",
    "model=lm(Y~D)\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
