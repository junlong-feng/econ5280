{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67932a03",
   "metadata": {},
   "source": [
    "# ECON5280 Lecture 3 Probability\n",
    "\n",
    "<font size=\"5\">Junlong Feng</font>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junlong-feng/econ5280/main?filepath=Lecture3_Probability.ipynb)\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Motivation: We treat our sample as random.\n",
    "* Probability distribution: Terminology and notation.\n",
    "* Expectation, variance, covariance and correlation: Basic tools.\n",
    "* Large sample theories: Advanced tools.\n",
    "\n",
    "## 1. Probability distribution.\n",
    "\n",
    "<font size=\"2\">  *We'll not adopt the rigorous approach to define random variable and probability distribution.*</font>\n",
    "\n",
    "### 1.1 Random Variable\n",
    "\n",
    "* A (real) random variable $X$ can take value from a set $\\Omega\\subset \\mathbb{R}$. Which value it equals is random.\n",
    "  * If the set $\\Omega$ is discrete, e.g. $\\{1,2,3\\}$, $X$ is **discrete**. If $\\Omega$ is a continuum, $X$ is **continuous**.\n",
    "* A subset of $\\Omega$ is called an **event**. \n",
    "  + For instance, let $\\Omega=\\{1,2,3\\}$ represent tomorrow's weather: $1=rainy$, $2=sunny$ and $3=other$. Then $\\{1\\}$ is the event that tomorrow is rainy. $\\{1,2\\}$ means that tomorrow is rainy or sunny.\n",
    "  + If two events cannot happen simultaneously, we say they are **mutually exclusive**.\n",
    "* Probability (measure) is a nonnegative **function** mapping the set of all possible events (the $\\sigma$-algebra generated by $\\Omega$ if you prefer a more rigorous treatment) to interval $[0,1]$.\n",
    "  + It needs to satisfy certain properties for such a nonnegative function to be a probability measure. But to save time, this is left for further reading and is not required.\n",
    "\n",
    "### 1.2 Probability Function, Cumulative Distribution Function, and Density\n",
    "\n",
    "How do we describe the behavior of a random variable? For a non-random variable, as long as we know its value, we know everything about it. For a random variable, since it takes on multiple values, we need to know two things: i) All possible values that it can take, and ii) (roughly) the probability of each singleton event.\n",
    "\n",
    "#### 1.2.1 Discrete Random Variables and Probability Function \n",
    "\n",
    "We start from the simpliest case where $\\Omega$ is a discrete set. Let $\\Omega\\equiv \\{x_{1},\\ldots,x_{J}\\}$. A singleton event is an event that only contains one element in $\\Omega$, e.g. $\\{x_{2}\\}$. \n",
    "\n",
    "The probability distribution of $X$ is the set of $\\Pr(X=x_{j})$ for all $j=1,...,J$. Once knowing the distribution, we can calculate the probability of any event. For instance, $\\Pr(X=x_{1}\\ or\\ x_{2})=\\Pr(X=x_{1})+\\Pr(X=x_{2})$.\n",
    "\n",
    "A useful discrete distribution:\n",
    "\n",
    "* Bernoulli, denoted by $Ber(p)$: $\\Omega$ only contains $2$ elements, usually normalized to $\\{0,1\\}$. $\\Pr(X=1)=p$.\n",
    "\n",
    "  * Bernoulli distribution describes any random variable that has two outcomes: You get a job at Morgan Stanley next year or not, Covid-19 ends next year or not, etc.\n",
    "\n",
    "  * In R, you can generate $n$ Bernoulli-distributed random variables by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "p=0.5\n",
    "## The second argument is fixed at 1. If you input a larger number, it will be a binomial distribution which we do not talk about in this course.\n",
    "X=rbinom(n,1,p) \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52ca0a",
   "metadata": {},
   "source": [
    "#### 1.2.2 Continuous Random Variables, Cumulative Distribution Function, and Density\n",
    "\n",
    "When $\\Omega$ is a continuum, e.g. $[0,\\infty)$, $(-\\infty,\\infty)$, and $[0,1]$, we say $X$ is continuous. The first thing you should always remember is that the probability of a singleton event for a continuous $X$ is always 0:\n",
    "$$\n",
    "\\Pr(X=x)=0,\\forall x\\in\\Omega.\n",
    "$$\n",
    "Therefore, it makes no sense to define the distribution of a continuous random variable in this way; $X$ and $Y$ may follow different distributions but they both have 0 probability for any singleton event.\n",
    "\n",
    "* The reason is that probability is a measure. Let $\\Omega=[0,1]$ whose length is 1. A well-defined probability should be the length of a subset of $\\Omega$. Then what is the length of a singleton, say $\\{0.2\\}$? We know a point has zero length so the probability of that event is 0.\n",
    "\n",
    "However, the probability of events like $[0.2,0.3)$ or $(-\\infty,2)$ may have nonzero probability. As long as we can design a function knowing which enables us to calculate the probabilities of all possible events, we can use it to represent the distribution. One such a function is called the *cumulative distribution function* (CDF):\n",
    "$$\n",
    "F_{X}(x)\\equiv \\Pr(X\\leq x),\\forall x\\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "* For practical purposes, it does not matter whether you define the CDF as $\\Pr(X\\leq x)$ or $\\Pr(X<x)$ **for continuous random variables**. This is because $\\Pr(X\\leq x)=\\Pr(X<x\\text{ or }X=x)=\\Pr(X<x)+\\Pr(X=x)=\\Pr(X<x)$.\n",
    "* You can verify that once you know $F_X$, you can calculate the probability of any event:\n",
    "  * $\\Pr(X\\in (-1,3))=F_{X}(3)-F_{X}(-1)$. $\\Pr(X>2)=1-F_{X}(2)$.\n",
    "* $x$ in the definition **is not necessarily in $\\Omega$**. For example, if $\\Omega=[0,1]$, it means $\\Pr(X\\in [0,1])=1$, so $1=\\Pr(X\\in [0,1])\\leq \\Pr(X\\leq 2)=F_{X}(2) $ so $F_{X}(2)=1$, still well-defined.\n",
    "* $F_{X}(\\cdot)$ is a continuous function on $\\mathbb{R}$ if $X$ is continuous.\n",
    "\n",
    "If you compare the CDF of a continuous random variable and the probability of a discrete random variable, they are pretty different; You can indeed define CDF for a discrete random variable as well (but you need to be careful about the equality sign now):\n",
    "$$\n",
    "F_{X}(x)\\equiv\\Pr(X\\leq x)=\\sum_{x_{j}\\leq x}\\Pr(X=x_{j}),\\text{  $X$ is discrete}.\n",
    "$$\n",
    " It seems unfair: discrete random variables have two ways to descibe its distribution while continuous random variables only have one. To make it fair, we propose a concept that is more parallel to the singleton probability of discrete random variables: the density function.\n",
    "\n",
    "Roughly, the density is the derivative of the CDF and thus:\n",
    "$$\n",
    "F_{X}(x)\\equiv \\Pr(X\\leq x)=\\int_{s\\leq x}f_{X}(s)ds,\\text{    $X$ is continuous}.\n",
    "$$\n",
    "If you compare the two equations above, you can see density for a continuous $X$ plays a similar role as probability for a discrete $X$.\n",
    "\n",
    "However, from calculus, we know not all continuous functions are differentiable. So, unlike the always-existsing probability of a discrete random variable, not all continuous random variables have densities.\n",
    "\n",
    "* A continuous random variable whose density exists is called **absolutely continuous**.\n",
    "* *Cantor distribution* is a continuous distribution that does not have density.\n",
    "\n",
    "Just like CDF, if you know the density of a random variable, you can calculate any event of a random variable. So, density (when it exists) contains the same amount info as CDF, and thus is another way to represent a distribution.\n",
    "\n",
    "* $\\Pr(X\\in (a,b))=\\int_{a}^{b}f_{X}(s)ds$. $\\Pr(X=x)=\\int_{x}^{x}f_{X}(s)ds=0$ under $f_{X}(x)<\\infty$ .\n",
    "\n",
    "A continuous distribution we'll use throughout the semester:\n",
    "\n",
    "* Normal (or gaussian), denoted by $N(\\mu,\\sigma^{2})$. $\\Omega=(-\\infty,\\infty)$. $f_{X}(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^{2}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(latex2exp)\n",
    "library(ggpointdensity)\n",
    "\n",
    "## Normal densities\n",
    "x=seq(-7,7,length=100)\n",
    "f1=dnorm(x,mean=0,sd=1)\n",
    "f2=dnorm(x,mean=2,sd=1)\n",
    "f3=dnorm(x,mean=0,sd=2)\n",
    "\n",
    "## Put the three density function values and x into a dataset\n",
    "data=data.frame(f1=f1,f2=f2,f3=f3,x=x)\n",
    "\n",
    "## Plot the three densities in one graph.\n",
    "p=ggplot(data=data)+\n",
    "  geom_line(aes(x=x,y=f1,colour=\"N(0,1)\"))+\n",
    "  geom_line(aes(x=x,y=f2,colour=\"N(2,1)\"),linetype=\"dashed\")+\n",
    "  geom_line(aes(x=x,y=f3,colour=\"N(0,4)\"),linetype=\"dotdash\")+\n",
    "  xlab('x')+\n",
    "  ylab('density')+\n",
    "  labs(colour=\"Legend\")+\n",
    "  theme(\n",
    "    legend.position = c(.95, .95),\n",
    "    legend.justification = c(\"right\", \"top\"),\n",
    "    legend.box.just = \"right\",\n",
    "    legend.margin = margin(6, 6, 6, 6)\n",
    "  )\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6b210",
   "metadata": {},
   "source": [
    "### 1.3 Joint, Conditional and Marginal Distributions\n",
    "\n",
    "  For our purposes, we will only review these three concepts for a continuous variable whose density exists.\n",
    "\n",
    "  Let $X$ represent years of schooling and $Y$ monthly income. Both are treated as continuous for now,\n",
    "\n",
    "  * Years of schooling is sometimes treated as continuous and sometimes discrete. This is the art side of economics and applied econometrics.\n",
    "\n",
    "$\\Pr(X>12)$ is the probability of someone who had post-high school education. When we calculate this probability in practice, we can count the number of people with post-high school education **in the full population**, regardless of her/his income, and divide it by the number of people of this population.\n",
    "\n",
    "$\\Pr(Y>8K)$ is the probability of someone whose monthly income is above 8K. When we calculate this probability in practice, we can count the number of such people **in the full population**, regardless of her/his education level, and divide it by the number of people of this population.\n",
    "\n",
    "$\\Pr(X>12,Y>8K)$ is the probablity of someone who had post-high school education AND whose monthly income is above 8K. We need to count the number of people **in the full population** who meet the two criteria **at the same time**, and divide it by the number of people of this population.\n",
    "\n",
    "$\\Pr(X>12|Y>8K)$ is the probability of someone with income above 8K who had post-high school education. We need to count the number of people **in a sub-population** with income above 8K, and divide it by the number of people of this population.\n",
    "\n",
    "  * The first two are called **marginal probabilities**.\n",
    "  * The third one is the **joint probability**.\n",
    "  * The last one is a **conditional probability**.\n",
    "\n",
    "  We always have $Joint=Marginal\\times Conditional$. Below are some properties of the three:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  F_{X,Y}(x,y)\\equiv &\\Pr(X\\leq x, Y\\leq y)=\\Pr(X\\leq x|Y\\leq y)F_{Y}(y)=\\Pr(Y\\leq y|X\\leq x)F_{X}(x),\\\\\n",
    "  F_{Y|X}(y|x)\\equiv &\\Pr(Y\\leq y|X=x)\\equiv \\lim_{\\epsilon\\to 0}\\Pr\\left(Y\\leq y|X\\in (x-\\epsilon,x+\\epsilon)\\right), f_{Y|X}(y|x)=\\partial_{y}F_{Y|X}(y|x),\\\\\n",
    "  f_{X,Y}(x,y)\\equiv &f_{X|Y}(x|y)f_{Y}(y)=f_{Y|X}(y|x)f_{X}(x),\\\\\n",
    "  F_{X}(x)=&\\int_{y\\in\\Omega_{Y}}F_{X|Y}(x|y)f_{Y}(y)dy,\\\\\n",
    "  f_{X}(x)=&\\int_{y\\in\\Omega_{Y}}f_{X,Y}(x,y)dy.\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "  * The first and the third equations are Bayes rule. \n",
    "  * The second equation defines conditional probability and density when the contiditioning event has 0 probability.\n",
    "  * The last two equalities say that **the joint distribution always contains no less information than the marginals**. That is, you can always recover the marginals, and thus the conditionals, from the joint, but not always *vice versa*.\n",
    "\n",
    "An important concept is **independence**.\n",
    "\n",
    "**Definition**. $X$ and $Y$ are independent if and only if $F_{X,Y}(x,y)=F_{X}(x)F_{Y}(y)$ **for all $(x,y)$**. Or if they have densities, $X$ and $Y$ are independent if and only if  $f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)$ **for all** $(x,y)$.\n",
    "\n",
    "  * In this course, we denote independence by $\\perp$. $X\\perp Y$ is equivalent to $Y\\perp X$.\n",
    "  * If $X$ and $Y$ are independent, then their marginals contain the same info as the joint by definition.\n",
    "  * If $X$ and $Y$ are independent, $g(X)$ and $h(Y)$ are also independent for any measurable function $g$ and $h$.\n",
    "\n",
    "## 2. Expectation, Variance, Covariance and Correlation\n",
    "\n",
    "In this section we discuss both discrete and continuous random variables, but we will use the same notation: density $f_{X}(x)$ means the density function when $X$ is continuous and means $\\Pr(X=x)$ when $X$ is discrete. $\\int dx$ means integral when $X$ is continuous and means $\\sum_{j}$ when $X$ is discrete.\n",
    "\n",
    "### 2.1 Marginals\n",
    "\n",
    "  * Expectation (or mean): $\\mathbb{E}(X)\\equiv\\int_{\\Omega}xf_{X}(x)dx$. Sometimes denoted by $\\mu$.\n",
    "    * The probability-weighted average of all possible outcome.\n",
    "    * **Intuition:** Your expectation of some future outcome is always affected by the proabability of each possible event, seldom the simple average.\n",
    "    * $\\mathbb{E}(aX+bY)=a\\mathbb{E}(X)+b\\mathbb{E}(Y)$.\n",
    "    * $\\mathbb{E}(XY)\\neq \\mathbb{E}(X)\\mathbb{E}(Y)$ in general.\n",
    "  * Variance: $\\mathbb{V}(X)\\equiv \\mathbb{E}(X-\\mu_{X})^{2}$. Sometimes denoted by $\\sigma_{X}^{2}$.\n",
    "    * The probability-weighted average of the squared Euclidean distance from mean to each outcome. \n",
    "    * **Intuition:** It's a squared distance to the mean, so measures the dispersion, or equality, or risk.\n",
    "      * Consider countries A and B with equal mean household income. A has a larger income variance. What does this say about income inequality?\n",
    "    * A related concept is the standard deviation (s.d.) $\\sigma_{X}=\\sqrt{\\mathbb{V}(X)}$ .\n",
    "    * $\\mathbb{V}(X)=\\mathbb{E}(X^{2})-\\mu_{X}^{2}$.\n",
    "    * $\\mathbb{V}(aX)=a^{2}\\mathbb{V}(X)$.\n",
    "  * Covariance: $cov(X,Y)=cov(Y,X)\\equiv\\mathbb{E}(X-\\mu_{X})(Y-\\mu_{Y})$.\n",
    "    + **Intuition:** By how much do $X$ and $Y$ co-vary? One increases, the other increases as well? (Positive.) Decreases? (Negative.) Stays unchanged? (Zero).\n",
    "    + $cov(X,Y)=\\mathbb{E}(XY)-\\mu_{X}\\mu_{Y}$.\n",
    "    + $cov(aX,Y)=a\\cdot cov(X,Y)$, $cov(X,X)=\\mathbb{V}(X)$.\n",
    "    + $cov(aX+bY,Z)=a\\cdot cov(X,Z)+b\\cdot cov(Y,Z)$.\n",
    "    + $\\mathbb{V}(aX+bY)=a^{2}\\mathbb{V}(X)+b^{2}\\mathbb{V}(Y)+2ab\\cdot cov(X,Y)$.\n",
    "  * Correlation: $\\rho (X,Y)=\\rho(Y,X)\\equiv \\frac{cov(X,Y)}{\\sigma_{X}\\sigma_{Y}}$. Sometimes denoted by $\\rho_{XY}$.\n",
    "    + **Intuition:** A large covariance may be because $X$ and $Y$ co-vary a lot, or may be because $X$ and/or $Y$  themselves are volatile (large s.d.). So need to divide the latter to reflect their *net* comovement.\n",
    "    + $\\rho_{XY}\\in [-1,1]$. Can you prove it?\n",
    "\n",
    "All the above four quantities are i) deterministic and ii) built on expectations. However, expectation itself may not exist: the integral of $xf(x)$ may explode when $x\\in (-\\infty,\\infty)$.\n",
    "\n",
    "  * Fat-tail distributions are often a concern. Check out Cauchy distribution.\n",
    "\n",
    "### 2.2 Conditionals\n",
    "\n",
    "Conditional expectation is perhaps the most important quantity in this course.\n",
    "\n",
    "  * $\\mathbb{E}(Y|X=x)\\equiv \\int y f_{Y|X}(y|x)dy$.\n",
    "\n",
    "    * $x$ here is just a parameter; the integral does not do anything to $X$.\n",
    "    * On the other hand, the randomness of the variable in front of the condtional sign $|$ is eliminated by the integral.\n",
    "    * $\\mathbb{E}(Y|X=x)$ is a **nonrandom** number.\n",
    "\n",
    "  * $\\mathbb{E}(Y|X)\\equiv \\int yf_{Y|X}(y|X)dy$.\n",
    "\n",
    "    * Again, the expectation eliminates the randomness of $Y$ by the integral.\n",
    "    * **But, the randomness of $X$ is left untouched**.\n",
    "    * $\\mathbb{E}(Y|X)$ is a **random variable**. Its randomness **only comes from $X$**.\n",
    "\n",
    "  * $\\mathbb{E}(aY+bZ|X)=a\\mathbb{E}(Y|X)+b\\mathbb{E}(Z|X)$, but in general $\\mathbb{E}(Y|aX+bZ)\\neq \\mathbb{E}(Y|aX)+\\mathbb{E}(Y|bZ)$.\n",
    "\n",
    "  * **Law of iterated expectation**: \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    &\\text{(General form)}\\ \\ \\ \\ \\mathbb{E}[\\mathbb{E}(Y|f(X))|g(X)]=\\mathbb{E}[\\mathbb{E}(Y|g(X))|f(X)]=\\mathbb{E}(Y|g(X))\\ \\text{if $g$ is a function of $f$},\\\\\n",
    "    &\\text{(Special case 1)}\\ \\ \\ \\ \\mathbb{E}\\left[\\mathbb{E}(Y|X_{1},X_{2})|X_{1}\\right]=\\mathbb{E}\\left[\\mathbb{E}(Y|X_{1})|X_{1},X_{2}\\right]=\\mathbb{E}(Y|X_{1}),\\\\\n",
    "    &\\text{(Special case 2)}\\ \\ \\ \\ \\mathbb{E}\\left[\\mathbb{E}(Y|X)\\right]=\\mathbb{E}(Y).\\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    + **Intuition**: Coarse information set dominates. Taking unconditional expectation is to shrink a large information set (the probability cloud) to a single point (expectation is nonrandom). The conditioning variables tell you when the shrinkage should stop, i.e., randomness from the conditioning variables should be kept. $\\mathbb{E}\\left[\\mathbb{E}(Y|X_{1},X_{2})|X_{1}\\right]$ says you first keep randomness in $X_{1}$ and $X_{2}$ and then eliminate the randomness in $X_{2}$. $\\mathbb{E}\\left[\\mathbb{E}(Y|X_{1})|X_{1},X_{2}\\right]$ says you first keep randomness in $X_{1}$ only, but after that randomness of $X_{2}$ has already been eliminated, so although the second step, i.e., the outer expectation, asks you to keep randomness of $X_{1}$ and $X_{2}$, $X_{2}$ is already gone. So either way, only randomness of $X_{1}$ is kept, so they both equal $\\mathbb{E}(Y|X_{1})$. \n",
    "      + A useful application: $\\mathbb{E}(X|X)=X$.\n",
    "\n",
    "  * **Conditional mean independence** if $\\mathbb{E}(Y|X)=\\mathbb{E}(Y)$ almost surely.\n",
    "\n",
    "    * $\\mathbb{E}(Y|X)=E(Y)$ in general does not imply $\\mathbb{E}(X|Y)=\\mathbb{E}(X)$.\n",
    "\n",
    "### 2.3 Independence, Conditional Mean Independence, and Zero Correlation\n",
    "\n",
    "So far we have learned three different measures characterizing the relationship between two random variables. What is their relationship?\n",
    "\n",
    "We have\n",
    "$$\n",
    "\\begin{align*}\n",
    "X\\perp Y\\equiv Y\\perp X&\\implies \\mathbb{E}(Y|X)=\\mathbb{E}(Y)\\  \\& \\ \\mathbb{E}(X|Y)=\\mathbb{E}(X),\\\\\n",
    "\\mathbb{E}(Y|X)=\\mathbb{E}(Y)\\  \\text{or} \\ \\mathbb{E}(X|Y)=\\mathbb{E}(X)&\\implies \\rho(X,Y)=0\\Leftrightarrow cov(X,Y)=0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* A useful trick to prove the second implication:\n",
    "  $$\n",
    "  \\mathbb{E}(XY)=\\mathbb{E}[\\mathbb{E}(XY|X)]=\\mathbb{E}(X\\mathbb{E}(Y|X))=\\mathbb{E}(X\\mu_{Y})=\\mu_{X}\\mu_{Y}.\n",
    "  $$\n",
    "\n",
    "* A generalization which follows the same proof as above:\n",
    "  $$\n",
    "  \\mathbb{E}(Y|X)=\\mathbb{E}(Y)\\implies \\mathbb{E}(Y\\cdot g(X))=\\mathbb{E}(Y)\\mathbb{E}(g(X)).\n",
    "  $$\n",
    "\n",
    "A special example when the reverse is also true is joint-normal distribution.\n",
    "\n",
    "**Definition**. Suppose $X$ and $Y$ are normal and $aX+bY$ is also normal for all $a$ and $b$, then the joint distribution of $(X,Y)$ is joint-normal.\n",
    "\n",
    "**Theorem**. If $(X,Y)$ are joint-normal, then $\\rho_{XY}=0\\Leftrightarrow X\\perp Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ac841",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(mvtnorm)\n",
    "library(plotly)\n",
    "library(latex2exp)\n",
    "library(ggpointdensity)\n",
    "library(weights)\n",
    "\n",
    "x=seq(-4,4,length=100)\n",
    "y=seq(-4,4,length=100)\n",
    "## A joint normal distribution can be fully-characterized by mean, variance, and covariances\n",
    "sigma_pos=matrix(c(1,0.5,0.5,1),ncol=2) # X and Y positively correlated\n",
    "sigma_neg=matrix(c(1,-0.5,-0.5,1),ncol=2) # X and Y negatively correlated\n",
    "sigma_ind=matrix(c(1,0,0,1),ncol=2) # X and Y independent\n",
    "mu=c(0,0) # mean\n",
    "\n",
    "f_pos=function(x,y){dmvnorm(cbind(x,y),mu,sigma_pos)}\n",
    "f_neg=function(x,y){dmvnorm(cbind(x,y),mu,sigma_neg)}\n",
    "f_ind=function(x,y){dmvnorm(cbind(x,y),mu,sigma_ind)}\n",
    "z_pos=outer(x,y,f_pos)\n",
    "z_neg=outer(x,y,f_neg)\n",
    "z_ind=outer(x,y,f_ind)\n",
    "pp_pos=plot_ly(x=x,y=y,z=z_pos)%>% add_surface()\n",
    "pp_neg=plot_ly(x=x,y=y,z=z_neg)%>% add_surface()\n",
    "pp_ind=plot_ly(x=x,y=y,z=z_ind)%>% add_surface()\n",
    "pp_ind\n",
    "# pp_neg\n",
    "# pp_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b9ba4",
   "metadata": {},
   "source": [
    "### 2.4 Joint\n",
    "\n",
    "For a joint distribution of a vector of random variables $X=(X_{1},...,X_{k})'$, \n",
    "\n",
    "* The mean $\\mu_{X}$ is a $k\\times 1$ vector $(\\mu_{X_{1}},\\ldots,\\mu_{X_{k}})'$. \n",
    "* The variance $\\mathbb{V}(X)$ (usually called the *variance-covariance matrix* or simply *covariance matrix*) is defined as $\\mathbb{E}[(X-\\mu_{X})(X-\\mu_{X})']$. It is a $k\\times k$ symmetric positive semi-definite matrix: (why?)\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\\sigma_{X_{1}}^{2}&cov(X_{1},X_{2})&\\cdots&cov(X_{1},X_{k})\\\\\n",
    "cov(X_{2},X_{1})&\\sigma_{X_{2}}^{2}&\\cdots&cov(X_{2},X_{k})\\\\\n",
    "&\\cdots&\\cdots&\\\\\n",
    "cov(X_{k},X_{1})&cov(X_{k},X_{2})&\\cdots&\\sigma_{X_{k}}^{2}\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "+ Can you come up with a sufficient condition so that the covariance matrix is positive definite?\n",
    "\n",
    "## 3. Large Sample Theory\n",
    "\n",
    "For almost the entire semester (except for panel data), we will assume our sample $\\{Y_{i},X_{i}\\}_{i=1,...,n}$ to be *i.i.d.*, that is, **identically and independently distributed**.\n",
    "\n",
    "* i.i.d. is assumed **across $i$**, that is, we assume Alice and Bob are independent; knowing Alice's income, age education does not help us to know Bob's income, age and education.\n",
    "* i.i.d. is **NOT** assumed **within** $i$, that is, Alice's age and education are allowed to be correlated with Alice's income.\n",
    "* Formally, $(Y_{i},X_{i})\\perp (Y_{j},X_{j})$ for all $i\\neq j$.\n",
    "\n",
    "In this semester, we will **never** make any assumptions about the exact distribution of $(Y_{i},X_{i})$. For instance, you will never hear me saying that income is normally distributed etc. Then how do we handle the randomness? What's the point of learning all those probability theories?\n",
    "\n",
    "* Recall the definition of mean, variance, covariance.... They are all built on $\\mathbb{E}$, which is built on the density. Without knowing the exact distribution of the sample, we don't know the density, and thus we don't know the exact value of any of the quantities we introduced.\n",
    "\n",
    "Luckly, it turns out regardless of the *true* distribution of the sample, which is unknown, we can approximate the mean of a sample by its sample average, and approximate the distribution by standard normal distribution very well, as long as the sample size is sufficiently large. The former result is called *the weak law of large number* (WLLN) and the latter is called *the central limit theorem* (CLT).\n",
    "\n",
    "### 3.1 WLLN\n",
    "\n",
    "For an arbitrary vector $X$, define its Euclidean distance from 0 by $\\|X\\|\\equiv \\sqrt{X'X}$.\n",
    "\n",
    "**Definition (Convergence in Probability)**. A sequence of $k\\times 1$ random vector $W_{n}$ is said to converge to a vector $w$ **in probability** if  **for all ** $\\delta>0$, $\\lim_{n\\to\\infty}\\Pr(\\|W_{n}-w\\|<\\delta)\\to 1$. Vector $w$ is called the **probability limit**, or p-limit of $W_{n}$. Convergence in probability is denoted by $W_{n}\\to_{p}w$ or equivalently $(W_{n}-w)\\to_{p}0$. It is also called *$W_{n}$ is consistent of $w$*.\n",
    "\n",
    "* If $W_{n}$ is not random, we define convergence in calculus as follows: for any $\\delta>0$, $\\|W_{n}-w\\|<\\delta$ for sufficiently large $n$.\n",
    "* Now $W_{n}$ is random, $\\|W_{n}-w\\|<\\delta$ is an event, not necessarily happen. However, consistency says the chance that it happens is larger and larger when $n$ is larger and larger.\n",
    "\n",
    "**Theorem (WLLN)**. Suppose we a $k\\times 1$ random vector with $n$ i.i.d. observations: $(X_{ij})$, $i=1,...,n$, $j=1,...,k$. Suppose for each $j=1,...,k$, $\\mathbb{V}(X_{ij})=\\sigma_{j}^{2}<\\infty$, then the $k\\times 1$ vector $\\bar{X}\\equiv (\\bar{X}_{1},...,\\bar{X}_{k})'$ satisfies $\\bar{X}\\to_{p}\\mu_{X}$ where $\\mu_{X}\\equiv (\\mu_{1},...,\\mu_{k})'$.\t\n",
    "\n",
    "Before we move on, what is the intuition behind WLLN? For simplicity, suppose $k=1$. \n",
    "\n",
    "* First, WLLN says a random variable is close to nonrandom number. How is that possible? What makes a variable nonrandom?\n",
    "  * 0 variance.\n",
    "* Then, what is the variance of $\\bar{X}$?\n",
    "  * $\\mathbb{V}(\\bar{X})=\\mathbb{V}\\left(\\frac{X_{1}+\\cdots +X_{n}}{n}\\right)=\\frac{1}{n^{2}}(n\\sigma_{X}^{2})=\\frac{\\sigma_{X}^{2}}{n}\\to 0$.\n",
    "* So, the fundamental driving force of convergence in probability in WLLN is that the variance of $\\bar{X}$ shrinks to 0 as $n$ goes to $\\infty$. In this way, the random $\\bar{X}$ becomes more and more like a nonrandom number as $n$ increases.\n",
    "* Now, what will happen if I multiply $\\bar{X}$ by $\\sqrt{n}$? You can verify that the sequence's variance will stay constant no matter what $n$ is. This brings up our next theorem, CLT.\n",
    "\n",
    "### 3.2 CLT\n",
    "\n",
    "**Definition (Convergence in Distribution)**. A sequence of $k\\times 1$ random vector $W_{n}$ is said to be converging to a random vector $W$ **in distribution** if $\\Pr(W_{n}\\in A)\\to \\Pr(W\\in A)$ for all $A\\subseteq \\mathbb{R}^{k}$. Convergence in distribution is denoted by $W_{n}\\to_{d}W$.\n",
    "\n",
    "* The convergence in the definition is deterministic: It's convergence of a nonrandom function $\\Pr$ to another nonrandom function.\n",
    "\n",
    "**Theorem (CLT)**. Suppose we a $k\\times 1$ random vector with $n$ i.i.d. observations: $(X_{ij})$, $i=1,...,n$, $j=1,...,k$. Suppose for each $j=1,...,k$, $\\mathbb{V}(X_{ij})=\\sigma_{j}^{2}<\\infty$, then the $k\\times 1$ vector $\\bar{X}\\equiv (\\bar{X}_{1},...,\\bar{X}_{k})'$ satisfies $\\sqrt{n}(\\bar{X}-\\mu_{X})\\to_{d}N(0,\\Sigma)$ where $\\mu_{X}\\equiv (\\mu_{1},...,\\mu_{k})'$, $\\Sigma\\equiv \\mathbb{V}(X_{i})$ is the $k\\times k$ covariance matrix.\n",
    "\n",
    "* CLT says that if we scale up the random sequence of the sample average by $\\sqrt{n}$, the sequence will converge to standard normal.\n",
    "  * From the last part of [Section 3.1](#3.1 WLLN), this is because the variance no longer shrinks to 0.\n",
    "  * A formal proof is demanding. Be prepared to see characteristic function/moment generating function in the proof. Not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(scales)\n",
    "library(ggplot2)\n",
    "library(mvtnorm)\n",
    "library(latex2exp)\n",
    "library(ggpointdensity)\n",
    "library(weights)\n",
    "## WLLN\n",
    "set.seed(12)\n",
    "e=0.1\n",
    "m=0\n",
    "n=0\n",
    "  for (b in 1:200){\n",
    "      m[1+10*(b-1)]=mean(rbinom(10,1,0.5))\n",
    "      m[2+10*(b-1)]=mean(rbinom(20,1,0.5))\n",
    "      m[3+10*(b-1)]=mean(rbinom(30,1,0.5))\n",
    "      m[4+10*(b-1)]=mean(rbinom(40,1,0.5))\n",
    "      m[5+10*(b-1)]=mean(rbinom(50,1,0.5))\n",
    "      m[6+10*(b-1)]=mean(rbinom(60,1,0.5))\n",
    "      m[7+10*(b-1)]=mean(rbinom(70,1,0.5))\n",
    "      m[8+10*(b-1)]=mean(rbinom(80,1,0.5))\n",
    "      m[9+10*(b-1)]=mean(rbinom(90,1,0.5))\n",
    "      m[10+10*(b-1)]=mean(rbinom(100,1,0.5))\n",
    "         \n",
    "      n[1+10*(b-1)]=10\n",
    "      n[2+10*(b-1)]=20\n",
    "      n[3+10*(b-1)]=30\n",
    "      n[4+10*(b-1)]=40\n",
    "      n[5+10*(b-1)]=50\n",
    "      n[6+10*(b-1)]=60\n",
    "      n[7+10*(b-1)]=70\n",
    "      n[8+10*(b-1)]=80\n",
    "      n[9+10*(b-1)]=90\n",
    "      n[10+10*(b-1)]=100\n",
    "      \n",
    "}\n",
    "data1=data.frame(m=m,n=n)\n",
    "data2=data1[order(n),]\n",
    "mm=data2$m\n",
    "prop=0\n",
    "for (i in 1:10){\n",
    "  prop[i]=sum((abs(mm[(1+200*(i-1)):(200+200*(i-1))]-0.5)<e))/200\n",
    "}\n",
    "#name=paste0(\"Pr(|μ-0.5|<ε)=\",prop,\", n=\",n)\n",
    "name=paste0(\"Pr=\",prop,\", n=\",n)\n",
    "datap=data.frame(data1,name)\n",
    "datap=datap[order(n),]\n",
    "\n",
    "## Figure 1 for WLLN\n",
    "ggplot(data1,aes(n,m))+geom_bin2d()+scale_fill_gradient(low = \"#efedf5\", high = \"#756bb1\",trans=\"log10\")+\n",
    "scale_x_continuous(breaks = seq(0, 100, 10))+\n",
    "  geom_hline(yintercept = 0.4,linetype=\"dashed\")+\n",
    "  geom_hline(yintercept = 0.6,linetype=\"dashed\")+labs(colour=TeX('$|\\\\bar{X}-0.5|\\\\leq\\\\epsilon$'))+ylab(\"Sample averages\")\n",
    "\n",
    "## Figure 2 for WLLN\n",
    "ggplot(datap, aes(n, m)) +geom_bin2d()+scale_fill_gradient(low = \"#efedf5\", high = \"#756bb1\",trans=\"log10\")+\n",
    "  facet_wrap(~name, ncol=5)+geom_hline(yintercept = 0.4,linetype=\"dashed\")+\n",
    "  scale_x_continuous(breaks = seq(0, 100, 20))+\n",
    "  geom_hline(yintercept = 0.6,linetype=\"dashed\")+labs(colour=TeX('$|\\\\bar{X}-0.5|\\\\leq\\\\epsilon$'))+ylab(\"Sample averages\")\n",
    "\n",
    "### CLT\n",
    "namen=paste0(\"n=\",n)\n",
    "data3=data.frame(data1,namen) # passed to geom_histogram and stat_function\n",
    "data3$m=sqrt(data3$n)*(data3$m-0.5)/0.5\n",
    "data3$namen=factor(data3$namen,levels=c(\"n=10\",\"n=20\",\"n=30\",\"n=40\",\"n=50\",\"n=60\",\"n=70\",\"n=80\",\"n=90\",\"n=100\"))\n",
    "ggplot(data3, aes(m, color=namen)) +\n",
    "  theme_bw() +\n",
    "  geom_histogram(aes(y=..density..), binwidth = 0.5,\n",
    "                 color=\"white\",fill = \"#cdc7e0\", size = 1) +\n",
    "  stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1),\n",
    "                color = \"gray61\", size = 1)+\n",
    "  facet_wrap(~namen, ncol=5)+xlab(TeX('$\\\\frac{\\\\sqrt{n}(\\\\bar{X}-\\\\mu_{X})}{\\\\sigma_{X}}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e96b5d",
   "metadata": {},
   "source": [
    "### 3.3 Some Useful Properties.\n",
    "\n",
    "We will use the following properties from time to time. \n",
    "\n",
    "* If $X_{n}\\to_{p}X$, then $X_{n}\\to_{d}X$. \n",
    "* (Slutsky's theorem) If $X_{n}\\to _{d}X$ and $Y_{n}\\to_{p}y$ where $y$ is nonrandom. Then\n",
    "  * $X_{n}+Y_{n}\\to_{d}X+y$.\n",
    "  * $X_{n}Y_{n}\\to_{d}yX$\n",
    "  * $X_{n}/Y_{n}\\to_{d}X/y$ provided that $y\\neq 0$.\n",
    "* (Continuous mapping theorem, CMT) If function $g$ is continuous almost everywhere, then\n",
    "  * $X_{n}\\to_{d}X\\implies g(X_{n})\\to_{d}g(X)$.\n",
    "  * $X_{n}\\to_{p} X\\implies g(X_{n})\\to_{p}g(X)$. \n",
    "* (Delta method) Suppose $X_{n}$ is a $k\\times 1$ vector and $g:\\mathbb{R}^{k}\\mapsto\\mathbb{R}^{l}$, $\\sqrt{n}(X_{n}-X)\\to_{d}N(0,\\Sigma)\\implies\\sqrt{n}(g(X_{n})-g(X))\\to_{d}(0,\\partial_{X}'g(X)\\Sigma\\partial_{X}g(X))$, provided that the Jacobian $\\partial_{X}g(X)$ exists and is not zero.\n",
    "  * Don't worry if this looks too abstract. We'll see examples later when we study least squares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
