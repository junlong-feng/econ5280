{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71afdbd",
   "metadata": {},
   "source": [
    "# ECON5280 Lecture 2 Matrix Algebra              \n",
    "\n",
    "<font size=\"5\">Junlong Feng</font>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junlong-feng/econ5280/main?filepath=Lecture2_MatrixAlgebra.ipynb)\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Motivation: We use matrix to represent data.\n",
    "* Notation: Scalar, vector and matrix.\n",
    "* Addition and multiplication: Corner stones of all our calculations.\n",
    "* Invertibility: $\\neq 0$?\n",
    "* Eigenvalue and positive (semi-) definiteness: $>0(\\geq0)$ in matrix sense.\n",
    "* Matrix calculus: Taking derivatives.  \n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "Recall in the previous lecture, we \n",
    "\n",
    "* used an $n\\times 1$ column vector $Y$ to collect $n$ observations of the dependent variable, and a $n\\times k$ matrix $X$ to collect all $n$ observations of the $k$ independent variables, and\n",
    "* we said we are going to propose formulae based on $Y$ and $X$ to study the dice of God, so\n",
    "* it is crucial to understand basics about matrices before we understand our formulae built on them.\n",
    "\n",
    "## 2. Notation\n",
    "\n",
    "We will play with three types of objects:\n",
    "\n",
    "* Scalar: a number. \n",
    "* Vector: a list of numbers arranged in a column or a row. \n",
    "  * e.g. $A=\\begin{pmatrix}a_{1}\\\\a_{2}\\end{pmatrix}$, $B=(b_{1},b_{2})$. $a_{1},a_{2},b_{1},b_{2}$ are all scalars. $A$ is a $2\\times 1$ vector (reads two by one). $B$ is a $1\\times 2$ vector.\n",
    "  * All vectors are treated as columns in this course if not otherwise stated.\n",
    "* Matrix: a rectangle array of numbers. \n",
    "  * When we say a matrix is $n\\times k$, we mean there are $n$ rows and $k$ columns in it. \n",
    "  * e.g. $A=\\begin{pmatrix}a_{11}& a_{12}&a_{13}\\\\a_{21}&a_{22}&a_{23}\\end{pmatrix}$. $A$ is a $2\\times 3$ matrix. $a_{ij}$ denotes the **entry** in the $i$-th row and the $j$-th column, or we say $a_{ij}$ is the $(i,j)$-th entry in $A$, denoted by $A_{ij}$.\n",
    "\n",
    "A few remarks are in order: \n",
    "\n",
    "* All **numbers** in the above definitions can be real or complex, deterministic or **random**.\n",
    "* A scalar is a $1\\times 1$ matrix and a vector is a $?\\times 1$ (column) or $1\\times ?$ (row) matrix. \n",
    "* Hence, matrix provides a unified perspective to view all kinds of mathematical objects we will see.\n",
    "\n",
    "Sometimes it is convenient to represent a matrix in terms of the column or row vectors instead of entries. \n",
    "\n",
    "* e.g. $A=\\begin{pmatrix}a_{11}& a_{12}&a_{13}\\\\a_{21}&a_{22}&a_{23}\\end{pmatrix}$ can also be written as $A=(A_{1},A_{2},A_{3})$ where $A_{j}=\\begin{pmatrix}a_{1j}\\\\a_{2j}\\end{pmatrix}$ for all $j=1,2,3$.\n",
    "\n",
    "### 2.1 Transpose\n",
    "\n",
    "**Definition**. $A'$ is the transpose of $A$ if the $(i,j)$-th entry in $A'$ is the $(j,i)$-th entry in $A$ for all $i,j$.\n",
    "\n",
    "Remarks:\n",
    "\n",
    "* If $A$ is $n\\times k$, then $A'$ is $k\\times n$.\n",
    "* e.g. if $A=\\begin{pmatrix}a_{11}& a_{12}&a_{13}\\\\a_{21}&a_{22}&a_{23}\\end{pmatrix}$, then $A'=\\begin{pmatrix}a_{11}& a_{21}\\\\a_{12}&a_{22}\\\\a_{13}&a_{23}\\end{pmatrix}$.\n",
    "* Recall that we can write $A=(A_{1},A_{2},A_{3})$ where $A_{j}$ is the $j$-th column in $A$. Then you can verify that $A'=\\begin{pmatrix}A_{1}'\\\\A_{2}'\\\\A_{3}'\\end{pmatrix}$.\n",
    "\n",
    "### 2.2 Symmetry\n",
    "\n",
    "**Definition**. Matrix $A$ is symmetric if $A=A'$.\n",
    "\n",
    "**Definition**. An $n\\times k$ matrix $A$ is square if $n=k$.\n",
    "\n",
    "Remarks.\n",
    "\n",
    "* A symmetric matrix must be a square matrix.\n",
    "* Matrix $A$ is symmetric if and only if $A_{ij}=A_{ji}$ for all $i,j$.\n",
    "\n",
    "**Definition**. A symmetric matrix is **diagonal** if $A_{ij}=0$ for all $i\\neq j$. A diagonal matrix is an **identity** matrix if $A_{ii}=1$ for all $i$.\n",
    "\n",
    "Remarks.\n",
    "\n",
    "* Example of a diagonal matrix: $A=\\begin{pmatrix}2&0\\\\0&-4\\end{pmatrix}$.\n",
    "* Example of an identity matrix: $A=\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix}$.\n",
    "\n",
    "## 3. Addition and Multiplication\n",
    "\n",
    "### 3.1 Addition\n",
    "\n",
    "You can **only add up matrices of the same order**. For instance, if $A$ is $2\\times 3$ and $B=2\\times 3$, you can calculate their sum by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "A+B&=\\begin{pmatrix}a_{11}& a_{12}&a_{13}\\\\a_{21}&a_{22}&a_{23}\\end{pmatrix}+\\begin{pmatrix}b_{11}& b_{12}&b_{13}\\\\b_{21}&b_{22}&b_{23}\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}a_{11}+b_{11}& a_{12}+b_{12}&a_{13}+b_{13}\\\\a_{21}+b_{21}&a_{22}+b_{22}&a_{23}+b_{23}\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### 3.2 Scalar Multiplication\n",
    "\n",
    "You can multiple a matrix (including vector and scalar) by a scalar: $cA=Ac=(cA_{ij}).$\n",
    "\n",
    "### 3.3 Multiplication of Matrices\n",
    "\n",
    "**Definition**. Two matrices $A$ and $B$ are **conformable** if the number of **columns** of $A$ is equal to the number of **rows** of $B$.\n",
    "\n",
    "* e.g. If $A$ is $k\\times r$ and $B$ is $r\\times s$, they are conformable. Their product is a $k\\times s$ matrix.\n",
    "\n",
    "How do we calculate matrix multiplication? Let's start with a simple case.\n",
    "\n",
    "**Multiplication of vectors: Inner product**. Let $a=(a_{1},a_{2},...,a_{k})'$ and $b=(b_{1},b_{2},...,b_{k})'$.\n",
    "\n",
    "* First check whether they are conformable. \n",
    "  * No.\n",
    "* What about $a'$ and $b$? \n",
    "  * Yes.\n",
    "* Define $a'b=a_{1}b_{1}+a_{2}b_{2}\\cdots a_{k}b_{k}\\equiv \\sum_{i=1}^{k}a_{i}b_{i}$.\n",
    "* Inner product of two vectors yields a scalar.\n",
    "\n",
    "**Multiplication of matrices: Representation I**. Let $A$ be $n\\times k$ and $B$ be $n\\times p$. Let $C\\equiv A'B$. We have known that $C$ is $k\\times p$. The $(a,b)$-th entry in $C$ is defined as:\n",
    "$$\n",
    "C_{ab}=\\sum_{i=1}^{n}A_{ia}B_{ib}.\n",
    "$$\n",
    "**Multiplication of matrices: Representation II**. Another representation is:\n",
    "$$\n",
    "C=A_{1}'B_{1}+A'_{2}B_{2}+\\cdots+ A'_{n} B_{n}=\\sum_{i=1}^{n}A'_{i}B_{i},\n",
    "$$\n",
    "where $A_{i}$ and $B_{i}$ are the $i$-th rows in $A$ and $B$ respectively. Each $A'_{i}B_{i}$ ($k\\times 1\\times 1\\times p$) is a $k\\times p$ matrix, so the sum is also $k\\times p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=matrix(c(1,2,3,4,5,6),3,2)\n",
    "B=matrix(c(2,3,4,5,6,7),3,2)\n",
    "## 1. Calculate AB by R\n",
    "t(A)%*%B\n",
    "## 2. Representation 1\n",
    "C1=matrix(0,2,2)\n",
    "for (i in 1:2){\n",
    "  for (j in 1:2){\n",
    "    C1[i,j]=sum(A[,i]*B[,j])\n",
    "  }\n",
    "}\n",
    "C1\n",
    "## 3. Representation 2\n",
    "C2=matrix(0,2,2)\n",
    "tA1=matrix(A[1,],2,1);tA2=matrix(A[2,],2,1)\n",
    "tA3=matrix(A[3,],2,1);B1=matrix(B[1,],1,2)\n",
    "B2=matrix(B[2,],1,2);B3=matrix(B[3,],1,2)\n",
    "C2=tA1%*%B1+tA2%*%B2+tA3%*%B3\n",
    "C2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ef88b",
   "metadata": {},
   "source": [
    "Some final remarks:\n",
    "\n",
    "* $A$ and $B$ are conformable **DOES NOT IMPLY** that $B$ and $A$ are conformable.\n",
    "* In the above example, $B$ and $A$ are not conformable if $p\\neq k$.\n",
    "* So, that $AB$ exists **DOES NOT IMPLY** that $BA$ also exists.\n",
    "* So of course $AB$ is **not necessarily** equal to $BA$. Matrix multiplication is **not commutative**.\n",
    "* However, it is associative and distributive: under conformability, we have\n",
    "\n",
    "$$\n",
    "A(BC)=ABC=(AB)C, \\text{\\ \\ }A(B+C)=AB+AC.\n",
    "$$\n",
    "\n",
    "* Another useful result: $(AB)'=B'A'$. No need to worry about conformability of $B'$ and $A'$; they must be conformable as long as $A$ and $B$ are.\n",
    "\n",
    "## 4. Invertibility\n",
    "\n",
    "Invertibility is only defined for square matrices. We denote the inverse of a matrix by $A^{-1}$. Like the scalar case, $A^{-1}A=AA^{-1}=I$, where $I$ is the identity matrix of the same order of $A$. \n",
    "\n",
    "However, not all square matrices are invertible. Consider a real scalar, i.e., a $1\\times 1$ real matrix $a$. Scalar $a$ is invertible (i.e., $1/a$ is well-defined) if and only if $a\\neq 0$. Unfortunately, this does not **directly** carry to matrix, though it does provides some insights.\n",
    "\n",
    "Let us start from a diagonal matrix $A=\\begin{pmatrix}a_{11}&0\\\\0&a_{22}\\end{pmatrix}$. You can think that this matrix represents a 2-D space: $a_{11}$ is the length of the $x$-direction and $a_{22}$ is the length of the $y$-direction. **$A$ is invertible if and only if both directions have nonzero length**.\n",
    "\n",
    "* Roughly speaking, a $k\\times k$ invertible matrix is such that none of its $k$-dimensions is *fake*; you cannot find a space of lower dimension to contain it.\n",
    "* Imagine $A=\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}$. This one is NOT invertible because it has a *fake* or *redundant dimension*; a 1-D space would have been enough to contain it.\n",
    "\n",
    "If we formalize this idea, this means none of the rows or columns of a matrix can be expressed as a linear combination of other rows or columns. Otherwise, the row or column that can be expressed by others is redundant. \n",
    "\n",
    "**Definition**. For a set of vectors, if none of them can be written as a linear combination of others, then they are **linearly independent**.\n",
    "\n",
    "**Definition**. The **rank** of a matrix is the number of linearly independent columns (or equivalently, rows). \n",
    "\n",
    "* By definition, for a $k\\times r$ matrix, it is always true that $rank(A)\\leq \\min(k,r)$. \n",
    "* A matrix is **full-rank** if $rank(A)= \\min(k,r)$.\n",
    "* A matrix $A$ is invertible **if and only if** $A$ is **square** and **full-rank**. \n",
    "\n",
    "Some useful results:\n",
    "\n",
    "* $rank(AB)\\leq \\min(rank(A),rank(B))$.\n",
    "* **IMPORTANT**. $rank(AA')=rank(A'A)=rank(A)$. Since $AA'$ and $A'A$ are square no matter whether $A$ itself is square, this means $AA'$ and $A'A$ are both invertible **if and only if** $A$ is full-rank.\n",
    "* If $A$ and $B$ are both invertible, then $(AB)^{-1}=B^{-1}A^{-1}$.\n",
    "* For an invertible diagonal $A=\\text{diag}(a_{ii})$, $A^{-1}=\\text{diag}(1/a_{ii})$.\n",
    "* For a $2\\times 2$ matrix $A=\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}$, its inverse has a simple analytical form: $A^{-1}=\\frac{1}{ad-bc}\\begin{pmatrix}d&-b\\\\-c&a\\end{pmatrix}$.\n",
    "\n",
    "## 5. Eigenvalue and Positive (Semi-)Definiteness\n",
    "\n",
    "For two real numbers $a$ and $b$, we know $a>b$ if and only if $a-b>0$. Like invertibility, can we borrow this idea and define how to compare two matrices?\n",
    "\n",
    "First, how do we define $a-b>0$? For any real number $c$, $a-b>0$ if and only if $c^2\\cdot (a-b)>0,\\forall c\\neq 0$. Similarly, $a-b\\geq 0$ if and only if $c^{2}\\cdot (a-b)\\geq 0,\\forall c$.\n",
    "\n",
    "We borrow this idea and define a *positive* matrix:\n",
    "\n",
    "**Definition.** A **symmetric** $k\\times k$ matrix $A$ is positive definite (p.d.) if for any $k\\times 1$ nonzero vector $x$, $x'Ax>0$. $A$ is positive semi-definite (p.s.d.) if for any $k\\times 1$ vector $x$, $x'Ax\\geq 0$.\n",
    "\n",
    "We can write $A>0$ if $A$ is p.d. and $A\\geq 0$ if $A$ is p.s.d. (We can also define negative definite (n.d.) and negative semi-definite (n.s.d.) similarly.)\n",
    "\n",
    "The definition is simple but not easy to use; it requires one to calculate $x'Ax$ for all possible $x$. Would prefer a more straightforward way to verify whether a given symmetric matrix is p.d. or p.s.d.\n",
    "\n",
    "Again, lets' start with a diagonal matrix $A=\\text{diag}(a_{ii}),i=1,...,k$. You can verify that for an arbitrary vector $x=(x_{1},...,x_{k})'$,\n",
    "$$\n",
    "x'Ax=\\sum_{i=1}^{k}x_{i}^{2}a_{ii}.\n",
    "$$\n",
    "Therefore, if $x'Ax>0$ all $x\\neq 0$, it has to be the case that $a_{ii}>0$ for all $i$. Similarly, $x'Ax\\geq 0$ for all $x$ if it $a_{ii}\\geq 0$ for all $i$.\n",
    "\n",
    "Now how about an arbitrary non-diagonal symmetric matrix? We have the following result:\n",
    "\n",
    "**Theorem (Spectral Decomposition)**. For a real $k\\times k$ symmetric matrix $A$, there exist a $k\\times k$ matrix $H$ and a diagonal matrix $\\Lambda$ such that i) $A=H\\Lambda H'$ and ii) $HH'=H'H=I$ where $I$ is the $k\\times k$ identity matrix.\n",
    "\n",
    "* For a square but **non-symmetric** matrix $A$, there exist an invertible $H$ and a diagonal matrix $\\Lambda$ such that $A=H\\Lambda H^{-1}$. The decomposition for symmetric matrices is a special case where $H^{-1}=H'$. This is why in that case $HH'=H'H=I$.\n",
    "* **Eigenvectors are not unique**. If all eigenvalues are distinct, eigenvectors are unique up to column-wise permutation column sign change.\n",
    "\n",
    "**Definition**. The diagonal elements in $\\Lambda$ are called **eigenvalues** of $A$. The column vectors in $H$ are called **eigenvectors** of $A$. Matrices $\\Lambda$ and $A$ are **similar**.\n",
    "\n",
    "By similarity, $A>(\\geq)0$ if and only if $\\Lambda>(\\geq)0$, and we have already known that $\\Lambda>(\\geq)0$ if and only if all its diagonal elements are $>(\\geq)0$. Therefore,\n",
    "\n",
    "* A symmetric real matrix $A$ is p.d. or p.s.d. if and only if all its eigenvalues are positive or non-negative.\n",
    "* One can verify that $AA'$ (and $A'A$) must be p.s.d. for any $A$ because $AA'=H\\Lambda H'H\\Lambda H=H\\Lambda^{2}H'$, and thus all the eigenvalues of $AA'$ are nonnegative. Moreover, if $A$ is full-rank (i.e., all diagonal elements in $\\Lambda$ are nonzero), then $AA'$ is p.d.\n",
    "\n",
    "Finally, spectral decomposition and matrix similarity can also help us to know the rank. We said a square diagonal matrix is full-rank if none of its dimensions is redundant, i.e., all the diagonal elements are nonzero. For a non-diagonal matrix, one can think about it as $rotating$ the non-redundant dimensions of a diagonal matrix, and thus it should maintain all the fundamental geometric properties. Indeed, the rank of a square matrix is the same as the rank of the diagonal matrix of its eigenvalues. Therefore:\n",
    "\n",
    "* The rank of a square matrix $A$ is equal to the number of nonzero eigenvalues.\n",
    "* A square matrix is invertible if and only if it is full-rank, or equivalently, if and only it none of its eigenvalues is zero.\n",
    "* Suppose $A$ is invertible. We know $A=H\\Lambda H^{-1}$. By the above point, $\\Lambda$ is invertible. Therefore, by the formula we learned in [Section 4](#4 Invertibility), $A^{-1}=H\\Lambda^{-1}H^{-1}$. Hence, the inverse of a matrix has the same eigenvectors as the original matrix, and each eigenvalue is the inverse of the corresponding original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Matrix)\n",
    "X=matrix(c(2,1,1,2),2,2) # you can try a matrix that is not full rank\n",
    "ed=eigen(X) # spectral decomposition\n",
    "rankMatrix(X) # calculate the rank; need package \"Matrix\"\n",
    "ed$values # call eigenvalues\n",
    "ed$vectors # call eigenvectors\n",
    "Xinv=solve(X) # taking inverse of a matrix\n",
    "edinv=eigen(Xinv)\n",
    "edinv$values\n",
    "edinv$vectors # eigen vectors are the same as X up to permutation and sign change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc23bb3",
   "metadata": {},
   "source": [
    "## 6 Calculus\n",
    "\n",
    "It's useful to know how to take derivatives of the following functions w.r.t. a vector:\n",
    "\n",
    "* For two vectors $a$ and $x$, $\\partial_{a}(x'a)=\\partial_{a}(a'x)=x$.\n",
    "* For vector $a$, $\\partial_{a}(a'a)=2a$.\n",
    "* For matrix $X$ and a conformable vector $\\beta$, $\\partial_{\\beta}X\\beta=X'$ .\n",
    "* Chain rule holds but you need to be careful about conformability: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\partial_{\\beta}(Y-X\\beta)'(Y-X\\beta)=&\\left(\\partial_{\\beta}(Y-X\\beta)\\right)\\cdot \\partial_{(Y-X\\beta)}\\left[(Y-X\\beta)'(Y-X\\beta)\\right]\\\\\n",
    "=&X'\\cdot 2(Y-X\\beta)\\\\\n",
    "=&2(X'Y-X'X\\beta).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Further Readings\n",
    "\n",
    "* Singular value and singular value decomposition (SVD) for rectangle matrices.\n",
    "* Matrix cookbook: <https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf>.\n",
    "* Trace and determinant.\n",
    "* Idempotent matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
