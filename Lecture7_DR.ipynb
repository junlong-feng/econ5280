{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22f872",
   "metadata": {},
   "source": [
    "# ECON5280 Chapter 7 Doubly Robust Learning\n",
    "\n",
    "<font size=\"5\">Junlong Feng</font>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junlong-feng/econ5280/main?filepath=Lecture7_DR.ipynb)\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Motivation: Can we estimate ATE under unconfoundedness? If yes, what's the best way?\n",
    "* OLS no longer works: ATE can be consistently estimated by OLS under complete randomization but that's no longer true under unconfoundedness in general.\n",
    "* Two ways to identify ATE: Expectation of CATE and inverse propensity score weighting.\n",
    "* Doubly Robust Learning: AIPW, the best way to estimate ATE under unconfoundedness.\n",
    "* Implementation: Everything can be done in R by a simple command.\n",
    "\n",
    "## 1. ATE under Unconfoundedness and the Failure of OLS\n",
    "\n",
    "In Chapter 5, we said if we have complete randomization, i.e.,\n",
    "$$\n",
    "D_{i}\\perp (Y_{i}(1),Y_{i}(0),W_{i}),\n",
    "$$\n",
    "we can estimate ATE of $D\\in \\{0,1\\}$ on $Y$ by linearly regressing $Y_{i}$ on $(1,D_{i})$ or $(1,D_{i},W_{i})$, regardless of the true model.\n",
    "\n",
    "In Chapter 6, we learned a relaxed assumption which may entertain a wider range of data. This weaker assumption is unconfoundedness:\n",
    "$$\n",
    "D_{i}\\perp (Y_{i}(1),Y_{i}(0))|W_{i}.\n",
    "$$\n",
    "In this chapter, we consider the following questions:\n",
    "\n",
    "- Is ATE identified under unconfoundedness?\n",
    "- If yes, can we still estimate it using OLS?\n",
    "- If yes to the first question but no to the second, then how should we estimate ATE in this case?\n",
    "\n",
    "The answer to the first question is obvious: Yes because we learned in Chapter 6 that under unconfoundedness, CATE is identified, and since ATE is the expectation of CATE, we can identify ATE as:\n",
    "$$\n",
    "ATE\\overset{unconfoundedness}{=}\\mathbb{E}[\\mathbb{E}(Y_{i}|D_{i}=1,W_{i})-\\mathbb{E}(Y_{i}|D_{i}=0,W_{i})].\n",
    "$$\n",
    "But how about the second question? Is regressing $Y$ onto $(1,D)$ or $(1,D,W)$ still a good idea? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90921ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "library(sandwich)\n",
    "set.seed(5280)\n",
    "## DGP\n",
    "n=5000\n",
    "w=rnorm(n,0,1)\n",
    "v=rnorm(n,0,1)\n",
    "d=((w+v)>0) ## So the treatment satisfies unconfoundedness but not complete randomization\n",
    "e=rnorm(n,0,1)\n",
    "y=1+d+w+d*w^2+sin(w)+e  ## So, ATE=2\n",
    "\n",
    "## OLS\n",
    "model1=lm(y~d)\n",
    "model2=lm(y~d+w)\n",
    "coeftest(model1,vcov.=vcovHC(model1,type=\"HC0\"))\n",
    "coeftest(model2,vcov.=vcovHC(model2,type=\"HC0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244eeca",
   "metadata": {},
   "source": [
    "From the simulation example, we can see that the estimates are not close to the true ATE. The reason is as follows. Consider the regression \n",
    "$$\n",
    "Y_{i}=\\beta_{0}+\\beta_{1}D_{i}+\\beta_{2}W_{i}+u_{i}.\n",
    "$$\n",
    "By the true model, $u_{i}=D_{i}W_{i}^{2}+sin(W_{i})+e_{i}$. By construction $D_{i}$ and $W_{i}$ are correlated, so $D_{i}$ and $u_{i}$ are correlated, leading to the failure of OLS/MM. However, we can verify that unconfoundedness actually holds in this case. \n",
    "\n",
    "To summarize: Under unconfoundedness, ATE is identified, so it is indeed possible to unbiasedly and consistently estimated it. Now the problem is that OLS/MM is no longer the proper estimator. We need to seek a different estimation approach. If we can find one, then we achieve something huge because ATE will not only be estimable under complete randomization: It can also be well-estimated under conditional randomization or even in observational data!\n",
    "\n",
    "## 2. Two Identification Equations for ATE\n",
    "\n",
    "It turns out that under unconfoudedness, ATE is not only identified by equation (3). We can identify it from a different angle.\n",
    "\n",
    "Since $D$ is binary, if $W$ and $D$ are correlated (this is the only interesting case because otherwise, we have complete randomization and thus don't need to worry about estimation), then $W$ only affects $D$ via the following conditional probability:\n",
    "$$\n",
    "p(W_{i})\\equiv\\Pr(D_{i}=1|W_{i}).\n",
    "$$\n",
    "This function $p(\\cdot)$ is called the **propensity score** of $D$. It tells us how $W_{i}$ affects the take-up probability of the treatment. In the special case of complete randomization, $p(W_{i})$ is a constant, not changing at all no matter what value $W_{i}$ takes.\n",
    "\n",
    "Since the propensity score is the only possible chanel for $W$ to affect $D$, if $D$ is independent of $(Y(1),Y(0))$ conditional on $W$, then one should expect to see that $D$ is independent of $(Y(1),Y(0))$ conditional on $p(W)$ as well. Indeed, this is a theorem and can be formally proved (Rosenbaum and Rubin, 1983).\n",
    "\n",
    "**Theorem**. If $D_{i}\\perp (Y_{i}(1),Y_{i}(0))|W_{i}$ and $D_{i}\\in\\{0,1\\}$, then $D_{i}\\perp (Y_{i}(1),Y_{i}(0))|p(W_{i})$.\n",
    "\n",
    "*Proof*. By the definition of independence, we are done if we can prove $\\Pr(D_{i}=1|Y_{i}(0),Y_{i}(1),p(W_{i}))=\\Pr(D_{i}=1|p(W_{i}))$ . Now,\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\Pr(D_{i}=1|Y_{i}(0),Y_{i}(1),p(W_{i}))\\\\\n",
    "=&\\mathbb{E}(D_{i}|Y_{i}(0),Y_{i}(1),p(W_{i}))\\\\\n",
    "\\overset{tower\\ property}{=}&\\mathbb{E}[\\mathbb{E}(D_{i}|Y_{i}(0),Y_{i}(1),W_{i})|Y_{i}(0),Y_{i}(1),p(W_{i})]\\\\\n",
    "\\overset{unconfoundedness}{=}&\\mathbb{E}[\\mathbb{E}(D_{i}|W_{i})|Y_{i}(0),Y_{i}(1),p(W_{i})]\\\\\n",
    "=&\\mathbb{E}(p(W_{i})|Y_{i}(0),Y_{i}(1),p(W_{i}))\\\\\n",
    "=&p(W_{i}),\n",
    "\\end{align*}\n",
    "$$\n",
    "where the penultimate equality is by $\\mathbb{E}(D_{i}|W_{i})=\\Pr(D_{i}=1|W_{i})\\equiv p(W_{i})$.\n",
    "\n",
    "Meanwhile, \n",
    "$$\n",
    "\\Pr(D_{i}=1|p(W_{i}))=\\mathbb{E}(D_{i}|p(W_{i}))=\\mathbb{E}[\\mathbb{E}(D_{i}|W_{i})|p(W_{i})]=p(W_{i}),\n",
    "$$\n",
    "where the second equality is by the tower property. Done.\n",
    "\n",
    "The beauty of this theorem is that, instead of controlling the whole vector $W_{i}$, which is multi-dimensional in general, one can simply condition on the propensity score, which is a scalar.\n",
    "\n",
    "Now we're ready to derive an alternative identification equation for ATE. The derivation of this new equation does not use the theorem above, but the form of this identification equation is a perfect example that knowing the propensity score is as good as knowing the whole $W_{i}$ vector.\n",
    "\n",
    "**Theorem**. Under unconfoundedness, if $p(W_{i})\\in (0,1)$ almost surely (which is called **overlap**), then\n",
    "$$\n",
    "ATE=\\mathbb{E}\\left(\\frac{D_{i}Y_{i}}{p(W_{i})}-\\frac{(1-D_{i})Y_{i}}{1-p(W_{i})}\\right).\n",
    "$$\n",
    "*Proof*. Note that $D_{i}Y_{i}=Y_{i}(1)$ if $D_{i}=1$ and $0$ if $D_{i}=0$. Therefore, $D_{i}Y_{i}=D_{i}Y_{i}(1)$. Similarly, $(1-D_{i})Y_{i}=(1-D_{i})Y_{i}(0)$ . Substitute them into the right hand side:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}\\left(\\frac{D_{i}Y_{i}}{p(W_{i})}-\\frac{(1-D_{i})Y_{i}}{1-p(W_{i})}\\right)\\\\\n",
    "=&\\mathbb{E}\\left(\\frac{D_{i}Y_{i}(1)}{p(W_{i})}\\right)-\\mathbb{E}\\left(\\frac{(1-D_{i})Y_{i}(0)}{1-p(W_{i})}\\right)\\\\\n",
    "\\overset{LIE}{=}&\\mathbb{E}\\left(\\frac{1}{p(W_{i})}\\mathbb{E}\\left(D_{i}Y_{i}(1)|W_{i}\\right)\\right)-\\mathbb{E}\\left(\\frac{1}{1-p(W_{i})}\\mathbb{E}\\left((1-D_{i})Y_{i}(0)|W_{i}\\right)\\right)\\\\\n",
    "\\overset{unconfoundedness}{=}&\\mathbb{E}\\left(\\frac{1}{p(W_{i})}\\mathbb{E}\\left(D_{i}|W_{i}\\right)\\mathbb{E}\\left(Y_{i}(1)|W_{i}\\right)\\right)-\\mathbb{E}\\left(\\frac{1}{1-p(W_{i})}\\mathbb{E}\\left((1-D_{i})|W_{i}\\right)\\mathbb{E}\\left(Y_{i}(0)|W_{i}\\right)\\right)\\\\\n",
    "=&\\mathbb{E}\\left(\\mathbb{E}\\left(Y_{i}(1)|W_{i}\\right)\\right)-\\mathbb{E}\\left(\\mathbb{E}\\left(Y_{i}(0)|W_{i}\\right)\\right)\\\\\n",
    "\\overset{by\\ eq.(3)}{=}&ATE.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So now, we can write ATE either as equation (3) or equation (7). This motivates us to estimate ATE by jointly using both.\n",
    "\n",
    "## 3 Augmented Inverse Propensity Score Weighting (AIPW) and Double Robustness \n",
    "\n",
    "### 3.1 An Oracle Estimator to Illustrate the Idea\n",
    "\n",
    "For simplicity, let $f_{d}(W_{i})\\equiv\\mathbb{E}(Y_{i}|D_{i}=d,W_{i})$ . Then equation (3) says\n",
    "$$\n",
    "\\tau\\equiv ATE=\\mathbb{E}(f_{1}(W_{i})-f_{0}(W_{i})).\n",
    "$$\n",
    "So it is natural to estimate ATE by\n",
    "$$\n",
    "\\hat{\\tau}^{*}_{ECATE}\\equiv\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{1}(W_{i})-f_{0}(W_{i})\\right).\n",
    "$$\n",
    "Of course, in practice, we would never observe $f_{1}$ and $f_{0}$. So $\\tau^{*}_{ECATE}$ is called an \"oracle\" estimator, an infeasible estimator where some true functions/parameters are involved. Oracle estimators are often helpful benchmarks to fix ideas and to focus on the main insight.\n",
    "\n",
    "Meanwhile, by equation (7), another oracle estimator of ATE could be\n",
    "$$\n",
    "\\hat{\\tau}^{*}_{IPW}\\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\frac{Y_{i}D_{i}}{p(W_{i})}-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{Y_{i}(1-D_{i})}{1-p(W_{i})}.\n",
    "$$\n",
    "$\\hat{\\tau}^{*}_{IPW}$ is also an oracle estimator because in most data, we do not known the true propensity score function $p$.\n",
    "\n",
    "Now, let's think about how to combine (9) and (10). We propose the following *infeasible augmented inverse propensity score weighted* estimator:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\tau}^{*}_{AIPW}\\equiv& \\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{1}(W_{i})-f_{0}(W_{i})\\right)\\\\\n",
    "&+\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{\\left(Y_{i}-f_{1}(W_{i})\\right)D_{i}}{p(W_{i})}-\\frac{\\left(Y_{i}-f_{0}(W_{i})\\right)(1-D_{i})}{1-p(W_{i})}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "This is very clever construction because it has two equivalent forms: ECATE+(roughly) mean zero error and IPW+(roughly) mean zero error:\n",
    "\n",
    "- In the current form, the first part is exactly $\\hat{\\tau}^{*}_{ECATE}$. The second part, instead of an estimator of ATE, is actually a weighted sum of two mean zero terms. For instance, by unconfoundedness,\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  &\\mathbb{E}\\left[\\left(Y_{i}-f_{1}(W_{i})\\right)D_{i}\\right]\\\\\n",
    "  =&\\mathbb{E}\\left[\\mathbb{E}\\left(\\left(Y_{i}-f_{1}(W_{i})\\right)D_{i}|W_{i}\\right)\\right]\\\\\n",
    "  =&\\mathbb{E}\\left[\\Pr(D_{i}=1|W_{i})\\mathbb{E}\\left(\\left(Y_{i}-f_{1}(W_{i})\\right)D_{i}|D_{i}=1,W_{i}\\right)\\right]\\\\\n",
    "  =&\\mathbb{E}\\left[\\Pr(D_{i}=1|W_{i})\\mathbb{E}\\left(\\left(Y_{i}-f_{1}(W_{i})\\right)|D_{i}=1,W_{i}\\right)\\right]\\\\\n",
    "  =&\\mathbb{E}\\left[\\Pr(D_{i}=1|W_{i})\\left(f_{1}(W_{i})-f_{1}(W_{i})\\right)\\right]\\\\\n",
    "  =&0.\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  - This suggests that, later on, when we need to estimate the **propensity score**, its estimation error would not much affect the estimator of ATE because the estimated propensity score inverse is multiplied by a mean zero term, leading to ignorable asymptotic bias. So **the propensity score NEEDS NOT to be consistently estimated as long as the conditional expectations $f_{d}$ are consistently estimated**.\n",
    "\n",
    "- Now here is the real magic: Like a transformer, $\\hat{\\tau}^{*}_{AIPW}$ has a second form. By simple algebra, we can transform $\\hat{\\tau}^{*}_{AIPW}$ into the following:\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\hat{\\tau}^{*}_{AIPW}=&\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{Y_{i}D_{i}}{p(W_{i})}-\\frac{Y_{i}(1-D_{i})}{1-p(W_{i})}\\right)\\\\\n",
    "  &+\\frac{1}{n}\\sum_{i=1}^{n}\\left(f_{1}(W_{i})\\left(1-\\frac{D_{i}}{p(W_{i})}\\right)-f_{0}(W_{i})\\left(1-\\frac{1-D_{i}}{1-p(W_{i})}\\right)\\right).\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  - The first term is exactly $\\hat{\\tau}_{IPW}^{*}$! The second term, instead of an estimator of ATE, is again a weighted sum of two mean zero terms. For instance,\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}\\left[1-\\frac{D_{i}}{p(W_{i})}\\right]=1-\\mathbb{E}\\left[\\frac{1}{p(W_{i})}\\mathbb{E}\\left(D_{i}|W_{i}\\right)\\right]=0.\n",
    "  $$\n",
    "\n",
    "  - This suggests that, later on, when we need to estimate the **conditional expectations**, their estimation errors would not much affect the estimator of ATE because the estimated conditional expectations are multiplied by mean zero terms, leading to ignorable asymptotic bias. So **the conditional expectations NEED NOT to be consistently estimated as long as the propensity score function is consistently estimated**.\n",
    "\n",
    "### 3.2 Weak Double Robustness\n",
    "\n",
    "Let $\\hat{f}_{d}$ and $\\hat{p}$ be estimators of $f_{d}$ and $p$. Now we can define a feasible estimator of ATE as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\tau}_{AIPW}\\equiv&\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{f}_{1}(W_{i})-\\hat{f}_{0}(W_{i})\\right)\\\\\n",
    "&+\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{\\left[Y_{i}-\\hat{f}_{1}(W_{i})\\right]D_{i}}{\\hat{p}(W_{i})}-\\frac{\\left[Y_{i}-\\hat{f}_{0}(W_{i})\\right](1-D_{i})}{1-\\hat{p}(W_{i})}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "By the analysis at the end of Section 3.1, we know that $\\hat{\\tau}_{AIPW}$ is **weakly doubly robust**: $\\hat{\\tau}_{AIPW}\\overset{p}{\\to}\\tau\\equiv ATE$ as long as one of $(\\hat{f}_{1},\\hat{f}_{0})$ and $\\hat{p}$ is consistent!\n",
    "\n",
    "This is a very interesting result. It says that to consistently estimate ATE under unconfoundedness, you only need to correctly estimate either the CATE function or the propensity score, and, most remarkably, you don't need to know which one is wrong. This is in sharp contrast to classical statistical/econometric methods where any mistake could result in a wrong final estimate.\n",
    "\n",
    "- If you heard about GMM, you know we can in fact estimate $\\tau$ by GMM since we have two moment equations ((3) and (7)) but only one parameter. However, GMM would be inconsistent if any of those two moment estimators is inconsistent.\n",
    "\n",
    "### 3.3 Strong Double Robustness and Double Machine Learning\n",
    "\n",
    "Weak double robustness only tells us consistency. We have a few concerns about the other aspects about $\\hat{\\tau}_{AIPW}$.\n",
    "\n",
    "- Unlike the infeasible AIPW $\\hat{\\tau}^{*}_{AIPW}$ where the estimator is simply constructed by some averages of i.i.d. random variables, the feasible version $\\hat{\\tau}_{AIPW}$ is constructed by the average of some estimated functions. Chapter 6 tells us that these estimators, no matter whether following the classical nonparametrics or the modern machine learning methods,  have a variance whose order is larger than $1/n$, resulting in a large standard error and a **slow rate of convergence** (i.e., although the estimator is consistent, it converges in probability to the true ATE slowly).\n",
    "- In data sciences, we usually have the belief of \"garbage in, garbage out\". Now that the input has a slow rate of convergence, one may expect to see that the final estimator also has a slow rate of convergence. Meanwhile, the variance of the final estimator might also be large because the variances of the nonparametrically or ML estimated $f_{d}$ and $p$ are large.\n",
    "\n",
    "However, in this section, we introduce a modified estimator based on our $\\hat{\\tau}_{AIPW}$ which achieves **strong double robustness**: The estimator's variance **has order 1/n** as long as the order of the variances of $\\hat{f}_{d}$ and $\\hat{p}$ are **not too large**. In other words, the estimator's rate of convergence achieves **the fastest speed** (i.e., the speed of $\\hat{\\tau}^{*}_{AIPW}$) as long as the convergence speed of $\\hat{f}_{d}$ and $\\hat{p}$ is **not too slow**.  In particular, **causal tree** and **causal forest** estimators of $f_{d}$ and $p$ satisfy the requirements.\n",
    "\n",
    "We now introduce this modified estimator. It is an example of **double machine learning** (DML), which uses the idea of **honest splitting** in causal trees. Recall that in the infeasible estimator $\\hat{\\tau}_{AIPW}^{*}$, we use data (more specifically the $W_{i}$s) to estimate ATE. However, in the feasible version $\\hat{\\tau}_{AIPW}$, the data are used twice for different purposes; they are first used to estimate the conditional expectations and propensity score, and then employed to construct the final estimate. Recall that we had a similar issue in the causal tree/forest estimator. So we can do the following honest splitting and **cross-fitting**:\n",
    "\n",
    "- Split your data set randomly into two halves $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$.\n",
    "\n",
    "- Use data in $\\mathcal{I}_{2}$ to estimate $f_{d}$ and $p$, by, for example, causal forest. Call them $\\hat{f}_{d}^{\\mathcal{I}_{2}}$ and $\\hat{p}^{\\mathcal{I}_{2}}$.\n",
    "\n",
    "- Plug in $W_{i}$ in $\\mathcal{I}_{1}$ into them and construct the following estimator. \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\hat{\\tau}^{\\mathcal{I}_{1}}\\equiv&\\frac{1}{|\\mathcal{I}_{1}|}\\sum_{i\\in\\mathcal{I}_{1}}\\left(\\hat{f}_{1}^{\\mathcal{I}_{2}}(W_{i})-\\hat{f}_{0}^{\\mathcal{I}_{2}}(W_{i})\\right)\\\\\n",
    "  &+\\frac{1}{|\\mathcal{I}_{1}|}\\sum_{i\\in\\mathcal{I}_{1}}\\left(\\frac{\\left[Y_{i}-\\hat{f}_{1}^{\\mathcal{I}_{2}}(W_{i})\\right]D_{i}}{\\hat{p}^{\\mathcal{I}_{2}}(W_{i})}-\\frac{\\left[Y_{i}-\\hat{f}_{0}^{\\mathcal{I}_{2}}(W_{i})\\right](1-D_{i})}{1-\\hat{p}^{\\mathcal{I}_{2}}(W_{i})}\\right).\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "- Now, swap the roles of $\\mathcal{I}_{1}$ and $\\mathcal{I}_{2}$ and construct $\\hat{\\tau}^{\\mathcal{I}_{2}}$. \n",
    "\n",
    "  - This step is called **cross-fitting**. It improves the stability of the final estimator because every data point is eventually used to do to jobs, estimating the nonparametric functions and estimating the ATE.\n",
    "\n",
    "- Construct our double machine learning estimator of ATE as\n",
    "  $$\n",
    "  \\hat{\\tau}^{DML}_{AIPW}\\equiv \\frac{|\\mathcal{I}_{1}|}{n}\\hat{\\tau}^{\\mathcal{I}_{1}}+\\frac{|\\mathcal{I}_{2}|}{n}\\hat{\\tau}^{\\mathcal{I}_{2}}.\n",
    "  $$\n",
    "\n",
    "It turns out $\\hat{\\tau}^{DML}_{AIPW}$  has a lot of nice properties under two assumptions: unconfoundedness and **strong overlap**. Strong overlap assumes that $p(W_{i})$ is not too close to 1 and 0 for all realizations of $W_{i}$. It means for every value of $W$, the proportion of people selecting either $D=1$ or $D=0$ is not too small. For instance, if $W_{i}$ means gender and $D_{i}$ means whether to get a master's degree, then strong overlap says that for each gender, there are not too few people who have a master's degree and not too few people who do not have one. \n",
    "\n",
    "Under unconfoundedness and strong overlap:\n",
    "\n",
    "- $\\hat{\\tau}^{DML}_{AIPW}$ is both weakly and strongly doubly robust. In particular, let the order of variance of $\\hat{f}_{d}$ be $1/n^{a}$ and that of $p$ be $1/n^{b}$. Then the variance of $\\hat{\\tau}^{DML}_{AIPW}$ has order $1/n$ as long as $a+b\\geq 1$, \n",
    "\n",
    "- Moreover, there is a positive number $V_{AIPW}$ such that\n",
    "  $$\n",
    "  \\sqrt{n}\\left(\\hat{\\tau}^{DML}_{AIPW}-\\tau\\right)\\overset{d}{\\to}\\mathcal{N}(0,V_{AIPW}).\n",
    "  $$\n",
    "\n",
    "- More impressively, among **a very large class of estimators** of ATE, $\\hat{\\tau}^{DML}_{AIPW}$ has **the smallest variance**.\n",
    "\n",
    "- To make things even better, $V_{AIPW}$ can be consistently estimated, so the standard error $se$ can be easily obtained by $\\sqrt{\\hat{V}_{AIPW}/n}$. Then, $t$-test, p-value and confidence interval can be constructed in the standard way.\n",
    "\n",
    "- To sum up, $\\hat{\\tau}^{DML}_{AIPW}$ is the fastest, most efficient, and most robust estimator of ATE under unconfoundedness. In short, it is the best. Always do it as long as you believe in you data unconfoundedness holds but you don't have complete randomization.\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "Because of the strong double robustness, you can choose most nonparametric/machine learning methods to estimate $f_{d}$ and $p$. Plugging them into the formula of $\\hat{\\tau}^{DML}_{AIPW}$ yields the desired estimator. For convenience, we recommend to estimate $f_{d}$ and $p$ by causal forest because one simple command **average_treatment_effect** in the grf package produces the estimated ATE and the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(grf)\n",
    "library(DiagrammeR)\n",
    "### Generate data. \n",
    "set.seed(5280)\n",
    "n <- 2000      \n",
    "p <- 10\n",
    "W <- matrix(rnorm(n * p), n, p)\n",
    "D <- rbinom(n, 1, 0.4 + 0.2 * (W[, 1] > 0))\n",
    "Y <- pmax(W[, 1], 0) * D + W[, 2] + pmin(W[, 3], 0) + rnorm(n)\n",
    "data=data.frame(Y,D,W)\n",
    "\n",
    "# Plant a forest\n",
    "tau.forest <- causal_forest(W, Y, D, num.trees = 4000)\n",
    "\n",
    "# Estimate the ATE\n",
    "average_treatment_effect(tau.forest,target.sample=\"all\",method=\"AIPW\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
